{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW08JFT0BUk1"
      },
      "source": [
        "# 컨볼루션 신경망(Convolution Neural Networks, CNN)\n",
        "\n",
        "- 완전 연결 네트워크의 문제점으로부터 시작\n",
        "\n",
        "  - 매개변수의 폭발적인 증가\n",
        "\n",
        "  - 공간 추론의 부족\n",
        "    - 픽셀 사이의 근접성 개념이 완전 연결 계층(Fully-Connected Layer)에서는 손실됨\n",
        "\n",
        "- 합성곱 계층은 입력 이미지가 커져도 튜닝해야 할 매개변수 개수에 영향을 주지 않음\n",
        "\n",
        "- 또한 그 어떠한 이미지에도 **그 차원 수와 상관없이** 적용될 수 있음\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png\">\n",
        "  \n",
        "  <center>[LeNet-5 구조]</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/@pechyonkin/key-deep-learning-architectures-lenet-5-6fc3c59e6f4</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORxbya83GHwc"
      },
      "source": [
        "## 컨볼루션 연산 (Convolution Operation)\n",
        "\n",
        "- 필터(filter) 연산\n",
        "  - 입력 데이터에 필터를 통한 어떠한 연산을 진행\n",
        "  \n",
        "  - **필터에 대응하는 원소끼리 곱하고, 그 합을 구함**\n",
        "\n",
        "  - 연산이 완료된 결과 데이터를 **특징 맵(feature map)**이라 부름\n",
        "\n",
        "- 필터(filter)\n",
        "  - 커널(kernel)이라고도 칭함\n",
        "  \n",
        "  - 흔히 사진 어플에서 사용하는 '이미지 필터'와 비슷한 개념\n",
        "\n",
        "  - 필터의 사이즈는 \"거의 항상 홀수\"\n",
        "    - 짝수이면 패딩이 비대칭이 되어버림\n",
        "  \n",
        "    - 왼쪽, 오른쪽을 다르게 주어야함\n",
        "  \n",
        "    - 중심위치가 존재, 즉 구별된 하나의 픽셀(중심 픽셀)이 존재\n",
        "\n",
        "  - 필터의 학습 파라미터 개수는 입력 데이터의 크기와 상관없이 일정  \n",
        "    따라서, 과적합을 방지할 수 있음\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <br>\n",
        "\n",
        "- 연산 시각화\n",
        "  <img src=\"https://www.researchgate.net/profile/Ihab_S_Mohamed/publication/324165524/figure/fig3/AS:611103423860736@1522709818959/An-example-of-convolution-operation-in-2D-2.png\" width=\"500\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/An-example-of-convolution-operation-in-2D-2_fig3_324165524</sub>\n",
        "\n",
        "\n",
        "- 일반적으로, 합성곱 연산을 한 후의 데이터 사이즈는  \n",
        "  ### $\\quad (n-f+1) \\times (n-f+1)$\n",
        "    $n$: 입력 데이터의 크기  \n",
        "    $f$: 필터(커널)의 크기\n",
        "\n",
        "  <br>\n",
        "  \n",
        "  <img src=\"https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=\"400\">\n",
        "\n",
        "  위 예에서 입력 데이터 크기($n$)는 5, 필터의 크기($k$)는 3이므로  \n",
        "  출력 데이터의 크기는 $(5 - 3 + 1) = 3$\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <sub>[이미지 출처] https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-60rJIUG49O"
      },
      "source": [
        "## Convolution vs Cross Correlation (참고)\n",
        "\n",
        "- 실제로 머신러닝 분야에서 '합성곱'이라는 용어를 일반적으로 사용하고는 있지만  \n",
        "  여기서 말하는 합성곱 연산은 '수학적 용어'로는 **교차 상관 관계(cross-correlation)**이라고 볼 수 있음\n",
        "\n",
        "- 수학적으로 합성곱 연산은 필터를 '뒤집어서' 연산을 진행\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Comparison_convolution_correlation.svg/400px-Comparison_convolution_correlation.svg.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://en.wikipedia.org/wiki/Convolution</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-ekDsJwN2Y-"
      },
      "source": [
        "## 패딩(padding)과 스트라이드(stride)\n",
        "- 필터(커널) 사이즈과 함께 **입력 이미지와 출력 이미지의 사이즈를 결정**하기 위해 사용\n",
        "\n",
        "- 사용자가 결정할 수 있음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNisiLu2TY9z"
      },
      "source": [
        "\n",
        "### 패딩\n",
        "- 입력 데이터의 주변을 특정 값으로 채우는 기법\n",
        "  - 주로 0으로 많이 채움\n",
        "\n",
        "<br>\n",
        "\n",
        "- 출력 데이터의 크기\n",
        "  ### $\\quad (n+2p-f+1) \\times (n+2p-f+1)$\n",
        "  <br>\n",
        "\n",
        "  위 그림에서, 입력 데이터의 크기($n$)는 5, 필터의 크기($f$)는 4, 패딩값($p$)은 2이므로    \n",
        "  출력 데이터의 크기는 ($5 + 2\\times 2 - 4 + 1) = 6$\n",
        "\n",
        "<br>\n",
        "\n",
        "### 'valid' 와 'same'\n",
        "- 'valid'\n",
        "  - 패딩을 주지 않음\n",
        "  - padding=0\n",
        "    - 0으로 채워진 테두리가 아니라 패딩을 주지 않는다는 뜻!\n",
        "\n",
        "- 'same'\n",
        "  - 패딩을 주어 입력 이미지의 크기와 연산 후의 이미지 크기를 같게!\n",
        "\n",
        "  - 만약, 필터(커널)의 크기가 $k$ 이면,  \n",
        "    패딩의 크기는 $p = \\frac{k-1}{2}$ (단, <u>stride=1)</u>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlZ7zG6ON85J"
      },
      "source": [
        "\n",
        "\n",
        "### 스트라이드\n",
        "- 필터를 적용하는 간격을 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPcsND-0OCNm"
      },
      "source": [
        "## 출력 데이터의 크기\n",
        "\n",
        "## $\\qquad OH = \\frac{H + 2P - FH}{S} + 1 $\n",
        "## $\\qquad OW = \\frac{W + 2P - FW}{S} + 1 $\n",
        "\n",
        "- 입력 크기 : $(H, W)$\n",
        "\n",
        "- 필터 크기 : $(FH, FW)$\n",
        "\n",
        "- 출력 크기 : $(OH, OW)$\n",
        "\n",
        "- 패딩, 스트라이드 : $P, S$\n",
        "\n",
        "- (주의)\n",
        "  - 위 식의 값에서 $\\frac{H + 2P - FH}{S}$ 또는 $\\frac{W + 2P - FW}{S}$가 정수로 나누어 떨어지는 값이어야 한다.  \n",
        "  - 만약, 정수로 나누어 떨어지지 않으면  \n",
        "    패딩, 스트라이드값을 조정하여 정수로 나누어 떨어지게 해야!\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUeddu9eJ6fG"
      },
      "source": [
        "## 텐서플로우/케라스 메소드\n",
        "- 이미지 합성곱의 경우 기본적으로 저차원 API의 `tf.nn.conv2d()`를 사용\n",
        "  - `input` : 형상이 $(B, \\ H, \\ W, \\ D)$인 입력 이미지 배치\n",
        "\n",
        "  - `filter` : $N$개의 필터가 쌓여 형상이 $(k_H, \\ k_W, \\ D, \\ N)$ 인 텐서\n",
        "\n",
        "  - `strides` : 보폭을 나타내는 4개의 정수 리스트.  \n",
        "    $\\qquad \\qquad [1, \\ S_H, \\ S_W, \\ 1]$ 을 사용\n",
        "\n",
        "  - `padding` : 패딩을 나타내는 4x2개의 정수 리스트나 사전 정의된 패딩 중 무엇을 사용할지 정의  \n",
        "    \"VALID\" or \"SAME\" 문자열 사용\n",
        "\n",
        "  - `name` : 해당 연산을 식별하는 이름\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1-KCMignLPKb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "k, D, N = 3, 3, 16\n",
        "input_data = torch.randn(size=(N, D, 32, 32))\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, n_kernels=32, kernel_size=(3, 3), strides=1, padding=0):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        self.n_kernels = n_kernels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=input_data.size(1),\n",
        "            out_channels=self.n_kernels,\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.strides,\n",
        "            padding=self.padding\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.conv.bias) # type: ignore\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = F.relu(self.conv(x))\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 32, 30, 30])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SimpleCNN(n_kernels=32, kernel_size=(3, 3), strides=1, padding=0)\n",
        "\n",
        "output_data = model(input_data)\n",
        "\n",
        "output_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x4UoMbF8jJ9"
      },
      "source": [
        "## 풀링(Pooling)\n",
        "\n",
        "- 필터(커널) 사이즈 내에서 특정 값을 추출하는 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDiaO3XF8oC_"
      },
      "source": [
        "### 맥스 풀링(Max Pooling)\n",
        "- 가장 많이 사용되는 방법\n",
        "\n",
        "- 출력 데이터의 사이즈 계산은 컨볼루션 연산과 동일\n",
        "## $\\quad OH = \\frac{H + 2P - FH}{S} + 1 $\n",
        "## $\\quad OW = \\frac{W + 2P - FW}{S} + 1 $\n",
        "\n",
        "- 일반적으로 stride=2, kernel_size=2 를 통해  \n",
        "  **특징맵의 크기를 <u>절반으로 줄이는 역할</u>**\n",
        "\n",
        "- 모델이 물체의 주요한 특징을 학습할 수 있도록 해주며,  \n",
        "  컨볼루션 신경망이 이동 불변성 특성을 가지게 해줌\n",
        "  - 예를 들어, 아래의 그림에서 초록색 사각형 안에 있는  \n",
        "    2와 8의 위치를 바꾼다해도 맥스 풀링 연산은 8을 추출\n",
        "\n",
        "- 모델의 파라미터 개수를 줄여주고, 연산 속도를 빠르게 해줌\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"600\">\n",
        "\n",
        "  <sub>[이미지 출처] https://cs231n.github.io/convolutional-networks/</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4czHpHrW8qyb"
      },
      "source": [
        "### 평균 풀링(Avg Pooling)\n",
        "\n",
        "- 필터 내의 있는 픽셀값의 평균을 구하는 과정\n",
        "\n",
        "- 과거에 많이 사용, 요즘은 잘 사용되지 않는다.\n",
        "\n",
        "- 맥스풀링과 마찬가지로 stride=2, kernel_size=2 를 통해  \n",
        "  특징 맵의 사이즈를 줄이는 역할\n",
        "\n",
        "  <img src=\"https://www.researchgate.net/profile/Juan_Pedro_Dominguez-Morales/publication/329885401/figure/fig21/AS:707709083062277@1545742402308/Average-pooling-example.png\" width=\"600\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/Average-pooling-example_fig21_329885401</sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NL27tG0uQDAt"
      },
      "outputs": [],
      "source": [
        "k, D, N = 3, 3, 16\n",
        "input_data = torch.randn(size=(N, D, 32, 32))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_kernels, kernel_size, pool_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_kernels = n_kernels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.k_strides = 1\n",
        "        self.k_padding = 0\n",
        "        \n",
        "        self.pool_size = (2, 2)\n",
        "        \n",
        "        self.p_strides = 2\n",
        "        self.p_padding = 0\n",
        "        \n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=input_data.size(1),\n",
        "            out_channels=n_kernels,\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.k_strides,\n",
        "            padding=self.k_padding\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.conv.bias) # type: ignore\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qWw49mAzQOJa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 32, 15, 15])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Net(n_kernels=32, kernel_size=(3, 3), pool_size=(2, 2))\n",
        "\n",
        "output_data = model(input_data)\n",
        "\n",
        "output_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmhhig5YQViL"
      },
      "source": [
        "## 완전 연결 계층(Fully-Connected Layer)\n",
        "\n",
        "- 입력으로 받은 텐서를 1차원으로 평면화(flatten) 함\n",
        "\n",
        "- 밀집 계층(Dense Layer)라고도 함\n",
        "\n",
        "- 일반적으로 분류기로서 **네트워크의 마지막 계층에서 사용**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JQ_ZMUwMQ9Zx"
      },
      "outputs": [],
      "source": [
        "N, D = 16, 32\n",
        "input_data = torch.randn(size=(N, D))\n",
        "\n",
        "class FullyConnectedLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.fc = nn.Linear(\n",
        "            in_features=self.input_size,\n",
        "            out_features=self.output_size\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self.fc.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        z = self.fc(x)\n",
        "        z = F.relu(z)\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kUHf1gnFQSLL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 10])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = FullyConnectedLayer(input_data.size(1), 10)\n",
        "\n",
        "output_data = model(input_data)\n",
        "\n",
        "output_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQDl4bYMSBgr"
      },
      "source": [
        "## 유효 수용 영역(ERF, Effective Receptive Field)\n",
        "\n",
        "- 입력 이미지에서 거리가 먼 요소를 상호 참조하여 결합하여 네트워크 능력에 영향을 줌\n",
        "\n",
        "- 입력 이미지의 영역을 정의해 주어진 계층을 위한 뉴런의 활성화에 영향을 미침\n",
        "\n",
        "- 한 계층의 필터 크기나 윈도우 크기로 불리기 때문에 RF(receptive field, 수용 영역)이라는 용어를 흔히 볼 수 있음\n",
        "\n",
        "  <img src=\"https://wiki.math.uwaterloo.ca/statwiki/images/8/8c/understanding_ERF_fig0.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://wiki.math.uwaterloo.ca/statwiki/index.php?title=Understanding_the_Effective_Receptive_Field_in_Deep_Convolutional_Neural_Networks</sub>\n",
        "\n",
        "<br>\n",
        "\n",
        "- RF의 중앙에 위치한 픽셀은 주변에 있는 픽셀보다 더 높은 가중치를 가짐\n",
        "  - 중앙부에 위치한 픽셀은 여러 개의 계층을 전파한 값\n",
        "\n",
        "  - 중앙부에 있는 픽셀은 주변에 위치한 픽셀보다 더 많은 정보를 가짐\n",
        "\n",
        "- 가우시안 분포를 따름\n",
        "\n",
        "  <img src=\"https://www.researchgate.net/publication/316950618/figure/fig4/AS:495826810007552@1495225731123/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks_fig4_316950618</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziTX7TwxVDIK"
      },
      "source": [
        "## CNN 구현\n",
        "\n",
        "### LeNet-5\n",
        "\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png\">\n",
        "  \n",
        "  <center>[LeNet-5 구조]</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/@pechyonkin/key-deep-learning-architectures-lenet-5-6fc3c59e6f4</sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OmHHY6NoRC6P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vzO3O0sucnsA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LeNet5(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (out): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # Layers\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=6,\n",
        "            kernel_size=(5, 5),\n",
        "            stride=1,\n",
        "            padding=0\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=6,\n",
        "            out_channels=16,\n",
        "            kernel_size=(5, 5),\n",
        "            stride=1,\n",
        "            padding=0\n",
        "        )\n",
        "        self.fc1 = nn.Linear(in_features=16*4*4, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.out = nn.Linear(in_features=84, out_features=self.n_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.max_pool2d(self.conv1(x), kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        x = F.max_pool2d(self.conv2(x), kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x\n",
        "        \n",
        "    @staticmethod\n",
        "    def init_params(m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = LeNet5(n_classes=10).to(device)\n",
        "model.apply(LeNet5.init_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MzpZgJdteQCA"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and pre-process data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_data = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "def calc_mean_std(loader: DataLoader):\n",
        "    sum, sq_sum, n_batches = 0, 0, 0\n",
        "    \n",
        "    for data, _ in loader:\n",
        "        sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        sq_sum += torch.mean(torch.square(data), dim=[0, 2, 3])\n",
        "        n_batches +=1\n",
        "    \n",
        "    mean = sum / n_batches\n",
        "    std = (sq_sum / n_batches - mean ** 2) ** .5 # V(X) = (E(X^2) - m^2)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "mean, std = calc_mean_std(train_loader)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean, ), (std, ))\n",
        "])\n",
        "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_preprocessed_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_preprocessed_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_fYNdTcwdd9R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.4808215450099174\n",
            "Epoch: 2, Loss: 0.12889249551486462\n",
            "Epoch: 3, Loss: 0.08721209748628292\n",
            "Epoch: 4, Loss: 0.06417326796403591\n",
            "Epoch: 5, Loss: 0.049191594218954124\n",
            "Epoch: 6, Loss: 0.04034546464681625\n",
            "Epoch: 7, Loss: 0.03085763002052269\n",
            "Epoch: 8, Loss: 0.0253040897451896\n",
            "Epoch: 9, Loss: 0.020658534286206866\n",
            "Epoch: 10, Loss: 0.016639426207922876\n",
            "Epoch: 11, Loss: 0.013389219945554245\n",
            "Epoch: 12, Loss: 0.014078267243631343\n",
            "Epoch: 13, Loss: 0.01292356733787567\n",
            "Epoch: 14, Loss: 0.013618171085266674\n",
            "Epoch: 15, Loss: 0.009890927400804085\n",
            "Epoch: 16, Loss: 0.008911232430577041\n",
            "Epoch: 17, Loss: 0.007950717733276295\n",
            "Epoch: 18, Loss: 0.012241137715581645\n",
            "Epoch: 19, Loss: 0.010861655385382077\n",
            "Epoch: 20, Loss: 0.007434515308829005\n",
            "Epoch: 21, Loss: 0.005452985707682835\n",
            "Epoch: 22, Loss: 0.01093680154512998\n",
            "Epoch: 23, Loss: 0.008235188857299286\n",
            "Epoch: 24, Loss: 0.004649671737415508\n",
            "Epoch: 25, Loss: 0.006964345298679\n",
            "Epoch: 26, Loss: 0.006614625240097744\n",
            "Epoch: 27, Loss: 0.0068850245736030045\n",
            "Epoch: 28, Loss: 0.008490608970873352\n",
            "Epoch: 29, Loss: 0.010161143952690837\n",
            "Epoch: 30, Loss: 0.009992365427024197\n",
            "Epoch: 31, Loss: 0.003807754828209154\n",
            "Epoch: 32, Loss: 0.00180489317684252\n",
            "Epoch: 33, Loss: 0.002281653411998203\n",
            "Epoch: 34, Loss: 0.002492954096243843\n",
            "Epoch: 35, Loss: 0.013161185205427365\n",
            "Epoch: 36, Loss: 0.009368694662593276\n",
            "Epoch: 37, Loss: 0.004302922773491987\n",
            "Epoch: 38, Loss: 0.0028488875818863\n",
            "Epoch: 39, Loss: 0.004124897343735164\n",
            "Epoch: 40, Loss: 0.003663871969829996\n",
            "Epoch: 41, Loss: 0.005128594054784774\n",
            "Epoch: 42, Loss: 0.010560534597363508\n",
            "Epoch: 43, Loss: 0.005974240479395604\n",
            "Epoch: 44, Loss: 0.00262824433564171\n",
            "Epoch: 45, Loss: 0.0012051607317717522\n",
            "Epoch: 46, Loss: 0.00220820921384152\n",
            "Epoch: 47, Loss: 0.0026040488653032955\n",
            "Epoch: 48, Loss: 0.00846582168247125\n",
            "Epoch: 49, Loss: 0.0140578727216033\n",
            "Epoch: 50, Loss: 0.004947942851978293\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "lr = 1e-3\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    running_loss = .0\n",
        "    \n",
        "    for idx, data in enumerate(train_preprocessed_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(inputs)\n",
        "        pred_vals, pred_indices = torch.max(outputs, dim=1)\n",
        "        loss = criterion(outputs, labels) # backpropagation 과정은 tensor 형태가 필요\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if idx == len(train_preprocessed_loader) - 1:\n",
        "            print(f'Epoch: {epoch + 1}, Loss: {running_loss / len(train_preprocessed_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rsHzPXwAdL0y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set\n",
            "Loss: 0.00039330, Accuracy: 98.56%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    t_loss = 0\n",
        "    \n",
        "    for data in test_preprocessed_loader:\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(inputs)\n",
        "        pred_vals, pred_indices = torch.max(outputs, dim=1)\n",
        "        t_loss += criterion(outputs, labels).item()\n",
        "        correct += pred_indices.eq(labels).sum().item()\n",
        "    \n",
        "    t_loss /= len(test_preprocessed_loader.dataset)\n",
        "    acc = correct / len(test_preprocessed_loader.dataset) * 100\n",
        "    print(\"Test Set\")    \n",
        "    print(f\"Loss: {t_loss:.8f}, Accuracy: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeMPNQRYoBW6"
      },
      "source": [
        "# Visual Geometry Group Net(VGGNet)\n",
        "\n",
        "- 2014년 ILSVRC 분류 과제에서 2등을 차지했지만, 이 후의 수많은 연구에 영향을 미침\n",
        "\n",
        "- 특징\n",
        "\n",
        "  - 활성화 함수로 `ReLU` 사용, Dropout 적용\n",
        "\n",
        "  - 합성곱과 풀링 계층으로 구성된 블록과 분류를 위한 완전 연결계층으로 결합된 전형적인 구조\n",
        "\n",
        "  - 인위적으로 데이터셋을 늘림\n",
        "    \n",
        "    - 이미지 변환, 좌우 반전 등의 변환을 시도\n",
        "\n",
        "  - 몇 개의 합성곱 계층과 최대-풀링 계층이 따르는 5개의 블록과,  \n",
        "    3개의 완전연결계층(학습 시, 드롭아웃 사용)으로 구성\n",
        "\n",
        "  - 모든 합성곱과 최대-풀링 계층에 `padding='SAME'` 적용\n",
        "\n",
        "  - 합성곱 계층에는 `stride=1`, 활성화 함수로 `ReLU` 사용\n",
        "\n",
        "  - 특징 맵 깊이를 증가시킴\n",
        "\n",
        "  - 척도 변경을 통한 데이터 보강(Data Augmentation)\n",
        "\n",
        "\n",
        "\n",
        "- 기여\n",
        "\n",
        "  - 3x3 커널을 갖는 두 합성곱 계층을 쌓은 스택이 5x5 커널을 갖는 하나의 합성곱 계층과 동일한 수용영역(ERF)을 가짐\n",
        "\n",
        "  - 11x11 사이즈의 필터 크기를 가지는 AlexNet과 비교하여,  \n",
        "    더 작은 합성곱 계층을 더 많이 포함해 더 큰 ERF를 얻음\n",
        "\n",
        "  - 이와 같이 합성곱 계층의 개수가 많아지면,  \n",
        "    **매개변수 개수를 줄이고, 비선형성을 증가시킴**\n",
        "\n",
        "\n",
        "- VGG-19 아키텍쳐\n",
        "\n",
        "  - VGG-16에 3개의 합성곱 계층을 추가\n",
        "\n",
        "  <br>   \n",
        "\n",
        "  <img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16.png\">\n",
        "  <center>VGG-16 아키텍쳐</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://neurohive.io/en/popular-networks/vgg16/ </sub>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- (참고) ILSVRC의 주요 분류 metric 중 하나는 `top-5`\n",
        "  \n",
        "  - 상위 5개 예측 안에 정확한 클래스가 포함되면 제대로 예측한 것으로 간주\n",
        "\n",
        "  - 일반적인 `top-k` metric의 특정 케이스\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ea52RHRWjSAs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "m1mnnxcilbdr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_train_test_dirs(source_dir, test_size=.2):\n",
        "    \"\"\"\n",
        "    폴더 구조를 변경하는 함수\n",
        "    \"\"\"\n",
        "    path = os.path.join(os.getcwd(), 'data', source_dir)\n",
        "    translate = {\"cane\": \"dog\", \"cavallo\": \"horse\", \"elefante\": \"elephant\", \"farfalla\": \"butterfly\", \"gallina\": \"chicken\", \"gatto\": \"cat\", \"mucca\": \"cow\", \"pecora\": \"sheep\", \"ragno\": \"spider\", \"scoiattolo\": \"squirrel\"}\n",
        "    classes = [dir for dir in os.listdir(path) if os.path.isdir(os.path.join(path, dir))]\n",
        "\n",
        "    for cls in classes:\n",
        "        new_cls = translate.get(cls, cls)\n",
        "        data_splits = ['train', 'test']\n",
        "        \n",
        "        for split in data_splits:\n",
        "            os.makedirs(os.path.join(path, split, new_cls), exist_ok=True)\n",
        "        \n",
        "        files = [f for f in os.listdir(os.path.join(path, cls)) if os.path.isfile(os.path.join(path, cls, f))]\n",
        "        train_files, test_files = train_test_split(files, test_size=test_size, random_state=42)\n",
        "\n",
        "        for split, files in zip(data_splits, (train_files, test_files)):\n",
        "            for f in files:\n",
        "                src_f_path = os.path.join(path, cls, f)\n",
        "                dst_f_path = os.path.join(path, split, new_cls, f)\n",
        "                \n",
        "                if os.path.exists(dst_f_path):\n",
        "                    os.remove(dst_f_path)\n",
        "                \n",
        "                shutil.move(src_f_path, dst_f_path)\n",
        "        \n",
        "        os.rmdir(os.path.join(path, cls))\n",
        "\n",
        "source_dir = 'animals-10'\n",
        "\n",
        "create_train_test_dirs(source_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fmQm2hVmUtD4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3890, 973)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cnt_imgs(source_dir, animal):\n",
        "    \"\"\"\n",
        "    제대로 분류 되었는지 확인하는 함수\n",
        "    \"\"\"\n",
        "    train_path = os.path.join(os.getcwd(), 'data', source_dir, 'train', animal)\n",
        "    test_path = os.path.join(os.getcwd(), 'data', source_dir, 'test', animal)\n",
        "\n",
        "    train_cnt = len([f for f in os.listdir(train_path) if os.path.isfile(os.path.join(train_path, f))])\n",
        "    test_cnt = len([f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))])\n",
        "    \n",
        "    return train_cnt, test_cnt\n",
        "\n",
        "train_cnt, test_cnt = cnt_imgs(source_dir, 'dog')\n",
        "\n",
        "train_cnt, test_cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGGNet16(nn.Module):\n",
        "    \"\"\"\n",
        "    output의 차원은 데이터 특성상 10개로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # Layers\n",
        "        self.conv1_1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=64,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv1_2 = nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=64,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv2_1 = nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=128,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv2_2 = nn.Conv2d(\n",
        "            in_channels=128,\n",
        "            out_channels=128,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv3_1 = nn.Conv2d(\n",
        "            in_channels=128,\n",
        "            out_channels=256,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv3_2 = nn.Conv2d(\n",
        "            in_channels=256,\n",
        "            out_channels=256,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv3_3 = nn.Conv2d(\n",
        "            in_channels=256,\n",
        "            out_channels=256,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv4_1 = nn.Conv2d(\n",
        "            in_channels=256,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv4_2 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv4_3 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv5_1 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv5_2 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv5_3 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "    \n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "\n",
        "        self.out = nn.Linear(4096, self.n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "    \n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        \n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        \n",
        "        x = F.relu(self.conv5_1(x))\n",
        "        x = F.relu(self.conv5_2(x))\n",
        "        x = F.relu(self.conv5_3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "\n",
        "        x = x.view(-1, self.n_flat_features(x))\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def n_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        n_features = 1\n",
        "        \n",
        "        for s in size:\n",
        "            n_features *= s\n",
        "        \n",
        "        return n_features\n",
        "    \n",
        "    @staticmethod\n",
        "    def init_params(m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "transform_temp = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root='./data/animals-10/train', transform=transform_temp)\n",
        "loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "def calc_mean_std(loader: DataLoader):\n",
        "    sum, sq_sum, n_batches = 0, 0, 0\n",
        "    \n",
        "    for data, _ in loader:\n",
        "        sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        sq_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
        "        n_batches += 1\n",
        "    \n",
        "    mean = sum / n_batches\n",
        "    std = ((sq_sum / n_batches) - mean ** 2) ** .5\n",
        "    \n",
        "    return mean, std\n",
        "\n",
        "mean, std = calc_mean_std(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=os.path.join(os.getcwd(), 'data', 'animals-10', 'train'), transform=transform)\n",
        "test_data = datasets.ImageFolder(root=os.path.join(os.getcwd(), 'data', 'animals-10', 'test'), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = VGGNet16().to('cuda')\n",
        "model.apply(VGGNet16.init_params)\n",
        "\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_loader: DataLoader, criterion: nn.modules.loss._Loss, optmizer: optim.Optimizer, epochs: int):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.\n",
        "        \n",
        "        for idx, (inputs, labels) in enumerate(train_loader):\n",
        "            optmizer.zero_grad()\n",
        "\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optmizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            if idx == len(train_loader) - 1:\n",
        "                print(f'Epoch: {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
        "                \n",
        "def test(model: nn.Module, test_loader: DataLoader, criterion: nn.modules.loss._Loss):\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        t_loss = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            pred_vals, pred_indices = torch.max(outputs, dim=1)\n",
        "            \n",
        "            t_loss += criterion(outputs, labels).item()\n",
        "            correct += pred_indices.eq(labels).sum().item()\n",
        "        \n",
        "        t_loss /= len(test_loader)\n",
        "        acc = correct / len(test_loader.dataset)\n",
        "    \n",
        "    print('Test')\n",
        "    print(f'Loss: {t_loss:.6f}, Accuracy: {acc * 100:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 10.962913176121603\n"
          ]
        }
      ],
      "source": [
        "train(model, train_loader, criterion, optimizer, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test(model, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMKkQqFUUVtP"
      },
      "source": [
        "# GoogLeNet, Inception 모듈\n",
        "\n",
        "- VGGNet을 제치고 같은 해 분류 과제에서 1등을 차지\n",
        "\n",
        "- 인셉션 블록이라는 개념을 도입하여, **인셉션 네트워크(Inception Network)**라고도 불림\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/2800/0*rbWRzjKvoGt9W3Mf.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5</sub>\n",
        "\n",
        "  <br>\n",
        "\n",
        "- 특징\n",
        "  \n",
        "  - CNN 계산 용량을 최적화하는 것을 고려\n",
        "\n",
        "  - 전형적인 합성곱, 풀링 계층으로 시작하고, 이 정보는 9개의 인셉션 모듈 스택을 통과  \n",
        "    해당 모듈을 하위 네트워크라고도 함\n",
        "\n",
        "  - 각 모듈에서 입력 특징 맵은 서로 다른 계층으로 구성된 4개의 병렬 하위 블록에 전달되고, 이를 서로 다시 연결\n",
        "\n",
        "  - 모든 합성곱과 풀링 계층의 padding옵션은 \"SAME\"이며 `stride=1`,  \n",
        "    활성화 함수는 `ReLU` 사용\n",
        "\n",
        "- 기여\n",
        "\n",
        "  - 규모가 큰 블록과 병목을 보편화\n",
        "\n",
        "  - 병목 계층으로 1x1 합성곱 계층 사용\n",
        "\n",
        "  - 완전 연결 계층 대신 풀링 계층 사용\n",
        "\n",
        "  - 중간 소실로 경사 소실 문제 해결\n",
        "\n",
        "  <img src=\"https://norman3.github.io/papers/images/google_inception/f01.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://norman3.github.io/papers/docs/google_inception.html</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okr3BazPYOpK"
      },
      "source": [
        "## 케라스로 Inception 모듈 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkrjUxOGTo-b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BajRJH3jYvLH"
      },
      "source": [
        "- 임의의 input_shape 값 지정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NQ-_jViYtxd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjf6NlAeYe5b"
      },
      "source": [
        "- 순차형 API 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8NSAg5tYauB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXM4AD1WZDO2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Xpkz1JZGcv"
      },
      "source": [
        "- 함수형 API 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R3LlxrXZEFJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Bex7TSZgaN"
      },
      "source": [
        "- 원시 버전의 인셉션 블록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVnpgzB4ZcSD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWgx32AvZnwm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm4_bmSmaaV9"
      },
      "source": [
        "## 케라스 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcLK4rlSZ1pj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_ryx4HCaoxA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P91oc7YFauuk"
      },
      "source": [
        "# ResNet - 잔차 네트워크\n",
        "\n",
        "- 네트워크의 깊이가 깊어질수록 경사가 소실되거나 폭발하는 문제를 해결하고자 함\n",
        "\n",
        "- 병목 합성곱 계층을 추가하거나 크기가 작은 커널을 사용\n",
        "\n",
        "- 152개의 훈련가능한 계층을 수직으로 연결하여 구성\n",
        "\n",
        "- 모든 합성곱과 풀링 계층에서 패딩옵셥으로 \"SAME\", stride=1 사용\n",
        "\n",
        "- 3x3 합성곱 계층 다음마다 배치 정규화 적용,  \n",
        "  1x1 합성곱 계층에는 활성화 함수가 존재하지 않음\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/1200/1*6hF97Upuqg_LdsqWY6n_wg.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIix2vrbwH9"
      },
      "source": [
        "## 잔차 블록 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SBaUMSfaqgH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9da3RqU_c4MZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1qyz3Jcds9Y"
      },
      "source": [
        "## 케라스 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6UTGl44drbG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCETH9aZd0V3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
