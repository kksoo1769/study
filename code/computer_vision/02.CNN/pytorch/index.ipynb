{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW08JFT0BUk1"
      },
      "source": [
        "# 컨볼루션 신경망(Convolution Neural Networks, CNN)\n",
        "\n",
        "- 완전 연결 네트워크의 문제점으로부터 시작\n",
        "\n",
        "  - 매개변수의 폭발적인 증가\n",
        "\n",
        "  - 공간 추론의 부족\n",
        "    - 픽셀 사이의 근접성 개념이 완전 연결 계층(Fully-Connected Layer)에서는 손실됨\n",
        "\n",
        "- 합성곱 계층은 입력 이미지가 커져도 튜닝해야 할 매개변수 개수에 영향을 주지 않음\n",
        "\n",
        "- 또한 그 어떠한 이미지에도 **그 차원 수와 상관없이** 적용될 수 있음\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png\">\n",
        "  \n",
        "  <center>[LeNet-5 구조]</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/@pechyonkin/key-deep-learning-architectures-lenet-5-6fc3c59e6f4</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORxbya83GHwc"
      },
      "source": [
        "## 컨볼루션 연산 (Convolution Operation)\n",
        "\n",
        "- 필터(filter) 연산\n",
        "  - 입력 데이터에 필터를 통한 어떠한 연산을 진행\n",
        "  \n",
        "  - **필터에 대응하는 원소끼리 곱하고, 그 합을 구함**\n",
        "\n",
        "  - 연산이 완료된 결과 데이터를 **특징 맵(feature map)**이라 부름\n",
        "\n",
        "- 필터(filter)\n",
        "  - 커널(kernel)이라고도 칭함\n",
        "  \n",
        "  - 흔히 사진 어플에서 사용하는 '이미지 필터'와 비슷한 개념\n",
        "\n",
        "  - 필터의 사이즈는 \"거의 항상 홀수\"\n",
        "    - 짝수이면 패딩이 비대칭이 되어버림\n",
        "  \n",
        "    - 왼쪽, 오른쪽을 다르게 주어야함\n",
        "  \n",
        "    - 중심위치가 존재, 즉 구별된 하나의 픽셀(중심 픽셀)이 존재\n",
        "\n",
        "  - 필터의 학습 파라미터 개수는 입력 데이터의 크기와 상관없이 일정  \n",
        "    따라서, 과적합을 방지할 수 있음\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <br>\n",
        "\n",
        "- 연산 시각화\n",
        "  <img src=\"https://www.researchgate.net/profile/Ihab_S_Mohamed/publication/324165524/figure/fig3/AS:611103423860736@1522709818959/An-example-of-convolution-operation-in-2D-2.png\" width=\"500\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/An-example-of-convolution-operation-in-2D-2_fig3_324165524</sub>\n",
        "\n",
        "\n",
        "- 일반적으로, 합성곱 연산을 한 후의 데이터 사이즈는  \n",
        "  ### $\\quad (n-f+1) \\times (n-f+1)$\n",
        "    $n$: 입력 데이터의 크기  \n",
        "    $f$: 필터(커널)의 크기\n",
        "\n",
        "  <br>\n",
        "  \n",
        "  <img src=\"https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=\"400\">\n",
        "\n",
        "  위 예에서 입력 데이터 크기($n$)는 5, 필터의 크기($k$)는 3이므로  \n",
        "  출력 데이터의 크기는 $(5 - 3 + 1) = 3$\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <sub>[이미지 출처] https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-60rJIUG49O"
      },
      "source": [
        "## Convolution vs Cross Correlation (참고)\n",
        "\n",
        "- 실제로 머신러닝 분야에서 '합성곱'이라는 용어를 일반적으로 사용하고는 있지만  \n",
        "  여기서 말하는 합성곱 연산은 '수학적 용어'로는 **교차 상관 관계(cross-correlation)**이라고 볼 수 있음\n",
        "\n",
        "- 수학적으로 합성곱 연산은 필터를 '뒤집어서' 연산을 진행\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Comparison_convolution_correlation.svg/400px-Comparison_convolution_correlation.svg.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://en.wikipedia.org/wiki/Convolution</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-ekDsJwN2Y-"
      },
      "source": [
        "## 패딩(padding)과 스트라이드(stride)\n",
        "- 필터(커널) 사이즈과 함께 **입력 이미지와 출력 이미지의 사이즈를 결정**하기 위해 사용\n",
        "\n",
        "- 사용자가 결정할 수 있음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNisiLu2TY9z"
      },
      "source": [
        "\n",
        "### 패딩\n",
        "- 입력 데이터의 주변을 특정 값으로 채우는 기법\n",
        "  - 주로 0으로 많이 채움\n",
        "\n",
        "<br>\n",
        "\n",
        "- 출력 데이터의 크기\n",
        "  ### $\\quad (n+2p-f+1) \\times (n+2p-f+1)$\n",
        "  <br>\n",
        "\n",
        "  위 그림에서, 입력 데이터의 크기($n$)는 5, 필터의 크기($f$)는 4, 패딩값($p$)은 2이므로    \n",
        "  출력 데이터의 크기는 ($5 + 2\\times 2 - 4 + 1) = 6$\n",
        "\n",
        "<br>\n",
        "\n",
        "### 'valid' 와 'same'\n",
        "- 'valid'\n",
        "  - 패딩을 주지 않음\n",
        "  - padding=0\n",
        "    - 0으로 채워진 테두리가 아니라 패딩을 주지 않는다는 뜻!\n",
        "\n",
        "- 'same'\n",
        "  - 패딩을 주어 입력 이미지의 크기와 연산 후의 이미지 크기를 같게!\n",
        "\n",
        "  - 만약, 필터(커널)의 크기가 $k$ 이면,  \n",
        "    패딩의 크기는 $p = \\frac{k-1}{2}$ (단, <u>stride=1)</u>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlZ7zG6ON85J"
      },
      "source": [
        "\n",
        "\n",
        "### 스트라이드\n",
        "- 필터를 적용하는 간격을 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPcsND-0OCNm"
      },
      "source": [
        "## 출력 데이터의 크기\n",
        "\n",
        "## $\\qquad OH = \\frac{H + 2P - FH}{S} + 1 $\n",
        "## $\\qquad OW = \\frac{W + 2P - FW}{S} + 1 $\n",
        "\n",
        "- 입력 크기 : $(H, W)$\n",
        "\n",
        "- 필터 크기 : $(FH, FW)$\n",
        "\n",
        "- 출력 크기 : $(OH, OW)$\n",
        "\n",
        "- 패딩, 스트라이드 : $P, S$\n",
        "\n",
        "- (주의)\n",
        "  - 위 식의 값에서 $\\frac{H + 2P - FH}{S}$ 또는 $\\frac{W + 2P - FW}{S}$가 정수로 나누어 떨어지는 값이어야 한다.  \n",
        "  - 만약, 정수로 나누어 떨어지지 않으면  \n",
        "    패딩, 스트라이드값을 조정하여 정수로 나누어 떨어지게 해야!\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUeddu9eJ6fG"
      },
      "source": [
        "## 텐서플로우/케라스 메소드\n",
        "- 이미지 합성곱의 경우 기본적으로 저차원 API의 `tf.nn.conv2d()`를 사용\n",
        "  - `input` : 형상이 $(B, \\ H, \\ W, \\ D)$인 입력 이미지 배치\n",
        "\n",
        "  - `filter` : $N$개의 필터가 쌓여 형상이 $(k_H, \\ k_W, \\ D, \\ N)$ 인 텐서\n",
        "\n",
        "  - `strides` : 보폭을 나타내는 4개의 정수 리스트.  \n",
        "    $\\qquad \\qquad [1, \\ S_H, \\ S_W, \\ 1]$ 을 사용\n",
        "\n",
        "  - `padding` : 패딩을 나타내는 4x2개의 정수 리스트나 사전 정의된 패딩 중 무엇을 사용할지 정의  \n",
        "    \"VALID\" or \"SAME\" 문자열 사용\n",
        "\n",
        "  - `name` : 해당 연산을 식별하는 이름\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "1-KCMignLPKb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "k, D, N = 3, 3, 16\n",
        "input_data = torch.randn(size=(N, D, 32, 32))\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, n_kernels=32, kernel_size=(3, 3), strides=1, padding=0):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        self.n_kernels = n_kernels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        \n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=input_data.size(1),\n",
        "            out_channels=self.n_kernels,\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.strides,\n",
        "            padding=self.padding\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.conv.bias) # type: ignore\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = F.relu(self.conv(x))\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 32, 30, 30])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SimpleCNN(n_kernels=32, kernel_size=(3, 3), strides=1, padding=0)\n",
        "\n",
        "output_data = model(input_data)\n",
        "\n",
        "output_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x4UoMbF8jJ9"
      },
      "source": [
        "## 풀링(Pooling)\n",
        "\n",
        "- 필터(커널) 사이즈 내에서 특정 값을 추출하는 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDiaO3XF8oC_"
      },
      "source": [
        "### 맥스 풀링(Max Pooling)\n",
        "- 가장 많이 사용되는 방법\n",
        "\n",
        "- 출력 데이터의 사이즈 계산은 컨볼루션 연산과 동일\n",
        "## $\\quad OH = \\frac{H + 2P - FH}{S} + 1 $\n",
        "## $\\quad OW = \\frac{W + 2P - FW}{S} + 1 $\n",
        "\n",
        "- 일반적으로 stride=2, kernel_size=2 를 통해  \n",
        "  **특징맵의 크기를 <u>절반으로 줄이는 역할</u>**\n",
        "\n",
        "- 모델이 물체의 주요한 특징을 학습할 수 있도록 해주며,  \n",
        "  컨볼루션 신경망이 이동 불변성 특성을 가지게 해줌\n",
        "  - 예를 들어, 아래의 그림에서 초록색 사각형 안에 있는  \n",
        "    2와 8의 위치를 바꾼다해도 맥스 풀링 연산은 8을 추출\n",
        "\n",
        "- 모델의 파라미터 개수를 줄여주고, 연산 속도를 빠르게 해줌\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"600\">\n",
        "\n",
        "  <sub>[이미지 출처] https://cs231n.github.io/convolutional-networks/</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4czHpHrW8qyb"
      },
      "source": [
        "### 평균 풀링(Avg Pooling)\n",
        "\n",
        "- 필터 내의 있는 픽셀값의 평균을 구하는 과정\n",
        "\n",
        "- 과거에 많이 사용, 요즘은 잘 사용되지 않는다.\n",
        "\n",
        "- 맥스풀링과 마찬가지로 stride=2, kernel_size=2 를 통해  \n",
        "  특징 맵의 사이즈를 줄이는 역할\n",
        "\n",
        "  <img src=\"https://www.researchgate.net/profile/Juan_Pedro_Dominguez-Morales/publication/329885401/figure/fig21/AS:707709083062277@1545742402308/Average-pooling-example.png\" width=\"600\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/Average-pooling-example_fig21_329885401</sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NL27tG0uQDAt"
      },
      "outputs": [],
      "source": [
        "k, D, N = 3, 3, 16\n",
        "input_data = torch.randn(size=(N, D, 32, 32))\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_kernels, kernel_size, pool_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_kernels = n_kernels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.k_strides = 1\n",
        "        self.k_padding = 0\n",
        "        \n",
        "        self.pool_size = (2, 2)\n",
        "        \n",
        "        self.p_strides = 2\n",
        "        self.p_padding = 0\n",
        "        \n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=input_data.size(1),\n",
        "            out_channels=n_kernels,\n",
        "            kernel_size=self.kernel_size,\n",
        "            stride=self.k_strides,\n",
        "            padding=self.k_padding\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.conv.bias) # type: ignore\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qWw49mAzQOJa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 32, 15, 15])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Net(n_kernels=32, kernel_size=(3, 3), pool_size=(2, 2))\n",
        "\n",
        "output_data = model(input_data)\n",
        "\n",
        "output_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmhhig5YQViL"
      },
      "source": [
        "## 완전 연결 계층(Fully-Connected Layer)\n",
        "\n",
        "- 입력으로 받은 텐서를 1차원으로 평면화(flatten) 함\n",
        "\n",
        "- 밀집 계층(Dense Layer)라고도 함\n",
        "\n",
        "- 일반적으로 분류기로서 **네트워크의 마지막 계층에서 사용**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "JQ_ZMUwMQ9Zx"
      },
      "outputs": [],
      "source": [
        "N, D = 16, 32\n",
        "input_data = torch.randn(size=(N, D))\n",
        "\n",
        "class FullyConnectedLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.fc = nn.Linear(\n",
        "            in_features=self.input_size,\n",
        "            out_features=self.output_size\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self.fc.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        z = self.fc(x)\n",
        "        z = F.relu(z)\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "kUHf1gnFQSLL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 10])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = FullyConnectedLayer(input_data.size(1), 10)\n",
        "\n",
        "output_data = model(input_data)\n",
        "\n",
        "output_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQDl4bYMSBgr"
      },
      "source": [
        "## 유효 수용 영역(ERF, Effective Receptive Field)\n",
        "\n",
        "- 입력 이미지에서 거리가 먼 요소를 상호 참조하여 결합하여 네트워크 능력에 영향을 줌\n",
        "\n",
        "- 입력 이미지의 영역을 정의해 주어진 계층을 위한 뉴런의 활성화에 영향을 미침\n",
        "\n",
        "- 한 계층의 필터 크기나 윈도우 크기로 불리기 때문에 RF(receptive field, 수용 영역)이라는 용어를 흔히 볼 수 있음\n",
        "\n",
        "  <img src=\"https://wiki.math.uwaterloo.ca/statwiki/images/8/8c/understanding_ERF_fig0.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://wiki.math.uwaterloo.ca/statwiki/index.php?title=Understanding_the_Effective_Receptive_Field_in_Deep_Convolutional_Neural_Networks</sub>\n",
        "\n",
        "<br>\n",
        "\n",
        "- RF의 중앙에 위치한 픽셀은 주변에 있는 픽셀보다 더 높은 가중치를 가짐\n",
        "  - 중앙부에 위치한 픽셀은 여러 개의 계층을 전파한 값\n",
        "\n",
        "  - 중앙부에 있는 픽셀은 주변에 위치한 픽셀보다 더 많은 정보를 가짐\n",
        "\n",
        "- 가우시안 분포를 따름\n",
        "\n",
        "  <img src=\"https://www.researchgate.net/publication/316950618/figure/fig4/AS:495826810007552@1495225731123/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://www.researchgate.net/figure/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks_fig4_316950618</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziTX7TwxVDIK"
      },
      "source": [
        "## CNN 구현\n",
        "\n",
        "### LeNet-5\n",
        "\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png\">\n",
        "  \n",
        "  <center>[LeNet-5 구조]</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/@pechyonkin/key-deep-learning-architectures-lenet-5-6fc3c59e6f4</sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "OmHHY6NoRC6P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vzO3O0sucnsA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LeNet5(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (out): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # Layers\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=6,\n",
        "            kernel_size=(5, 5),\n",
        "            stride=1,\n",
        "            padding=0\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=6,\n",
        "            out_channels=16,\n",
        "            kernel_size=(5, 5),\n",
        "            stride=1,\n",
        "            padding=0\n",
        "        )\n",
        "        self.fc1 = nn.Linear(in_features=16*4*4, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.out = nn.Linear(in_features=84, out_features=self.n_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.max_pool2d(self.conv1(x), kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        x = F.max_pool2d(self.conv2(x), kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "\n",
        "        return x\n",
        "        \n",
        "    @staticmethod\n",
        "    def init_params(m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = LeNet5(n_classes=10).to(device)\n",
        "model.apply(LeNet5.init_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "MzpZgJdteQCA"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and pre-process data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_data = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "def calc_mean_std(loader: DataLoader):\n",
        "    sum, sq_sum, n_batches = 0, 0, 0\n",
        "    \n",
        "    for data, _ in loader:\n",
        "        sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        sq_sum += torch.mean(torch.square(data), dim=[0, 2, 3])\n",
        "        n_batches +=1\n",
        "    \n",
        "    mean = sum / n_batches\n",
        "    std = (sq_sum / n_batches - mean ** 2) ** .5 # V(X) = (E(X^2) - m^2)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "mean, std = calc_mean_std(train_loader)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mean, ), (std, ))\n",
        "])\n",
        "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_preprocessed_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_preprocessed_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "_fYNdTcwdd9R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.557654636337402\n",
            "Epoch: 2, Loss: 0.13484441306679806\n",
            "Epoch: 3, Loss: 0.08652228806564148\n",
            "Epoch: 4, Loss: 0.06544912608062968\n",
            "Epoch: 5, Loss: 0.04937011444980794\n",
            "Epoch: 6, Loss: 0.03950584412532601\n",
            "Epoch: 7, Loss: 0.031849166804409405\n",
            "Epoch: 8, Loss: 0.02582009834574258\n",
            "Epoch: 9, Loss: 0.022043122213452736\n",
            "Epoch: 10, Loss: 0.01992389608689762\n",
            "Epoch: 11, Loss: 0.01564681690602385\n",
            "Epoch: 12, Loss: 0.01325382659994462\n",
            "Epoch: 13, Loss: 0.013788178145013591\n",
            "Epoch: 14, Loss: 0.012152602786118996\n",
            "Epoch: 15, Loss: 0.009027431737086637\n",
            "Epoch: 16, Loss: 0.010531126717947661\n",
            "Epoch: 17, Loss: 0.011664751093547315\n",
            "Epoch: 18, Loss: 0.015532196492719603\n",
            "Epoch: 19, Loss: 0.009587820614948174\n",
            "Epoch: 20, Loss: 0.005657295747957331\n",
            "Epoch: 21, Loss: 0.005274079769337054\n",
            "Epoch: 22, Loss: 0.007630581128227703\n",
            "Epoch: 23, Loss: 0.005413186255231886\n",
            "Epoch: 24, Loss: 0.0074952933670422855\n",
            "Epoch: 25, Loss: 0.011542059531022964\n",
            "Epoch: 26, Loss: 0.007396229544401843\n",
            "Epoch: 27, Loss: 0.005621461904585421\n",
            "Epoch: 28, Loss: 0.007541273915813555\n",
            "Epoch: 29, Loss: 0.004057106185606012\n",
            "Epoch: 30, Loss: 0.004201425754798309\n",
            "Epoch: 31, Loss: 0.003946126377733426\n",
            "Epoch: 32, Loss: 0.005375784021197568\n",
            "Epoch: 33, Loss: 0.008200786552225373\n",
            "Epoch: 34, Loss: 0.007978743826893054\n",
            "Epoch: 35, Loss: 0.00582246194377497\n",
            "Epoch: 36, Loss: 0.008190991869567834\n",
            "Epoch: 37, Loss: 0.007090601515200762\n",
            "Epoch: 38, Loss: 0.0050809131342480504\n",
            "Epoch: 39, Loss: 0.004505578222663044\n",
            "Epoch: 40, Loss: 0.0028640817652326113\n",
            "Epoch: 41, Loss: 0.0014736496122361828\n",
            "Epoch: 42, Loss: 0.0006502724833663183\n",
            "Epoch: 43, Loss: 0.008504581593218818\n",
            "Epoch: 44, Loss: 0.009621889338914696\n",
            "Epoch: 45, Loss: 0.00605396846103565\n",
            "Epoch: 46, Loss: 0.004181055584187339\n",
            "Epoch: 47, Loss: 0.0021789967431028886\n",
            "Epoch: 48, Loss: 0.0035209312564579995\n",
            "Epoch: 49, Loss: 0.003076638948250741\n",
            "Epoch: 50, Loss: 0.0030653269246342578\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "lr = 1e-3\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    running_loss = .0\n",
        "    \n",
        "    for idx, data in enumerate(train_preprocessed_loader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(inputs)\n",
        "        pred_vals, pred_indices = torch.max(outputs, dim=1)\n",
        "        loss = criterion(outputs, labels) # backpropagation 과정은 tensor 형태가 필요\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if idx == len(train_preprocessed_loader) - 1:\n",
        "            print(f'Epoch: {epoch + 1}, Loss: {running_loss / len(train_preprocessed_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "rsHzPXwAdL0y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set\n",
            "Loss: 0.00033505, Accuracy: 98.75%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    t_loss = 0\n",
        "    \n",
        "    for data in test_preprocessed_loader:\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(inputs)\n",
        "        pred_vals, pred_indices = torch.max(outputs, dim=1)\n",
        "        t_loss += criterion(outputs, labels).item()\n",
        "        correct += pred_indices.eq(labels).sum().item()\n",
        "    \n",
        "    t_loss /= len(test_preprocessed_loader.dataset)\n",
        "    acc = correct / len(test_preprocessed_loader.dataset) * 100\n",
        "    print(\"Test Set\")    \n",
        "    print(f\"Loss: {t_loss:.8f}, Accuracy: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeMPNQRYoBW6"
      },
      "source": [
        "# Visual Geometry Group Net(VGGNet)\n",
        "\n",
        "- 2014년 ILSVRC 분류 과제에서 2등을 차지했지만, 이 후의 수많은 연구에 영향을 미침\n",
        "\n",
        "- 특징\n",
        "\n",
        "  - 활성화 함수로 `ReLU` 사용, Dropout 적용\n",
        "\n",
        "  - 합성곱과 풀링 계층으로 구성된 블록과 분류를 위한 완전 연결계층으로 결합된 전형적인 구조\n",
        "\n",
        "  - 인위적으로 데이터셋을 늘림\n",
        "    \n",
        "    - 이미지 변환, 좌우 반전 등의 변환을 시도\n",
        "\n",
        "  - 몇 개의 합성곱 계층과 최대-풀링 계층이 따르는 5개의 블록과,  \n",
        "    3개의 완전연결계층(학습 시, 드롭아웃 사용)으로 구성\n",
        "\n",
        "  - 모든 합성곱과 최대-풀링 계층에 `padding='SAME'` 적용\n",
        "\n",
        "  - 합성곱 계층에는 `stride=1`, 활성화 함수로 `ReLU` 사용\n",
        "\n",
        "  - 특징 맵 깊이를 증가시킴\n",
        "\n",
        "  - 척도 변경을 통한 데이터 보강(Data Augmentation)\n",
        "\n",
        "\n",
        "\n",
        "- 기여\n",
        "\n",
        "  - 3x3 커널을 갖는 두 합성곱 계층을 쌓은 스택이 5x5 커널을 갖는 하나의 합성곱 계층과 동일한 수용영역(ERF)을 가짐\n",
        "\n",
        "  - 11x11 사이즈의 필터 크기를 가지는 AlexNet과 비교하여,  \n",
        "    더 작은 합성곱 계층을 더 많이 포함해 더 큰 ERF를 얻음\n",
        "\n",
        "  - 이와 같이 합성곱 계층의 개수가 많아지면,  \n",
        "    **매개변수 개수를 줄이고, 비선형성을 증가시킴**\n",
        "\n",
        "\n",
        "- VGG-19 아키텍쳐\n",
        "\n",
        "  - VGG-16에 3개의 합성곱 계층을 추가\n",
        "\n",
        "  <br>   \n",
        "\n",
        "  <img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16.png\">\n",
        "  <center>VGG-16 아키텍쳐</center>\n",
        "\n",
        "  <sub>[이미지 출처] https://neurohive.io/en/popular-networks/vgg16/ </sub>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- (참고) ILSVRC의 주요 분류 metric 중 하나는 `top-5`\n",
        "  \n",
        "  - 상위 5개 예측 안에 정확한 클래스가 포함되면 제대로 예측한 것으로 간주\n",
        "\n",
        "  - 일반적인 `top-k` metric의 특정 케이스\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Ea52RHRWjSAs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "m1mnnxcilbdr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_train_test_dirs(source_dir, test_size=.2):\n",
        "    \"\"\"\n",
        "    폴더 구조를 변경하는 함수\n",
        "    \"\"\"\n",
        "    path = os.path.join(os.getcwd(), 'data', source_dir)\n",
        "    translate = {\"cane\": \"dog\", \"cavallo\": \"horse\", \"elefante\": \"elephant\", \"farfalla\": \"butterfly\", \"gallina\": \"chicken\", \"gatto\": \"cat\", \"mucca\": \"cow\", \"pecora\": \"sheep\", \"ragno\": \"spider\", \"scoiattolo\": \"squirrel\"}\n",
        "    classes = [dir for dir in os.listdir(path) if os.path.isdir(os.path.join(path, dir))]\n",
        "\n",
        "    for cls in classes:\n",
        "        new_cls = translate.get(cls, cls)\n",
        "        data_splits = ['train', 'test']\n",
        "        \n",
        "        for split in data_splits:\n",
        "            os.makedirs(os.path.join(path, split, new_cls), exist_ok=True)\n",
        "        \n",
        "        files = [f for f in os.listdir(os.path.join(path, cls)) if os.path.isfile(os.path.join(path, cls, f))]\n",
        "        train_files, test_files = train_test_split(files, test_size=test_size, random_state=42)\n",
        "\n",
        "        for split, files in zip(data_splits, (train_files, test_files)):\n",
        "            for f in files:\n",
        "                src_f_path = os.path.join(path, cls, f)\n",
        "                dst_f_path = os.path.join(path, split, new_cls, f)\n",
        "                \n",
        "                if os.path.exists(dst_f_path):\n",
        "                    os.remove(dst_f_path)\n",
        "                \n",
        "                shutil.move(src_f_path, dst_f_path)\n",
        "        \n",
        "        os.rmdir(os.path.join(path, cls))\n",
        "\n",
        "source_dir = 'animals-10'\n",
        "\n",
        "create_train_test_dirs(source_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "fmQm2hVmUtD4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3890, 973)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cnt_imgs(source_dir, animal):\n",
        "    \"\"\"\n",
        "    제대로 분류 되었는지 확인하는 함수\n",
        "    \"\"\"\n",
        "    train_path = os.path.join(os.getcwd(), 'data', source_dir, 'train', animal)\n",
        "    test_path = os.path.join(os.getcwd(), 'data', source_dir, 'test', animal)\n",
        "\n",
        "    train_cnt = len([f for f in os.listdir(train_path) if os.path.isfile(os.path.join(train_path, f))])\n",
        "    test_cnt = len([f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))])\n",
        "    \n",
        "    return train_cnt, test_cnt\n",
        "\n",
        "train_cnt, test_cnt = cnt_imgs(source_dir, 'dog')\n",
        "\n",
        "train_cnt, test_cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGGNet16(nn.Module):\n",
        "    \"\"\"\n",
        "    output의 차원은 데이터 특성상 10개로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        \n",
        "        # Layers\n",
        "        self.conv1_1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=64,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv1_2 = nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=64,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv2_1 = nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=128,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv2_2 = nn.Conv2d(\n",
        "            in_channels=128,\n",
        "            out_channels=128,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv3_1 = nn.Conv2d(\n",
        "            in_channels=128,\n",
        "            out_channels=256,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv3_2 = nn.Conv2d(\n",
        "            in_channels=256,\n",
        "            out_channels=256,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv3_3 = nn.Conv2d(\n",
        "            in_channels=256,\n",
        "            out_channels=256,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv4_1 = nn.Conv2d(\n",
        "            in_channels=256,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv4_2 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv4_3 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        \n",
        "        self.conv5_1 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv5_2 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "        self.conv5_3 = nn.Conv2d(\n",
        "            in_channels=512,\n",
        "            out_channels=512,\n",
        "            kernel_size=(3, 3),\n",
        "            stride=(1, 1),\n",
        "            padding=(1, 1)\n",
        "        )\n",
        "    \n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "\n",
        "        self.out = nn.Linear(4096, self.n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "    \n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = F.relu(self.conv3_3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        \n",
        "        x = F.relu(self.conv4_1(x))\n",
        "        x = F.relu(self.conv4_2(x))\n",
        "        x = F.relu(self.conv4_3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "        \n",
        "        x = F.relu(self.conv5_1(x))\n",
        "        x = F.relu(self.conv5_2(x))\n",
        "        x = F.relu(self.conv5_3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
        "\n",
        "        x = x.view(-1, self.n_flat_features(x))\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def n_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        n_features = 1\n",
        "        \n",
        "        for s in size:\n",
        "            n_features *= s\n",
        "        \n",
        "        return n_features\n",
        "    \n",
        "    @staticmethod\n",
        "    def init_params(m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.5085, 0.4806, 0.3968]) tensor([0.2631, 0.2585, 0.2699])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "transform_temp = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root='./data/animals-10/train', transform=transform_temp)\n",
        "loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "def calc_mean_std(loader: DataLoader):\n",
        "    sum, sq_sum, n_batches = 0, 0, 0\n",
        "    \n",
        "    for data, _ in loader:\n",
        "        sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        sq_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
        "        n_batches += 1\n",
        "    \n",
        "    mean = sum / n_batches\n",
        "    std = ((sq_sum / n_batches) - mean ** 2) ** .5\n",
        "    \n",
        "    return mean, std\n",
        "\n",
        "mean, std = calc_mean_std(loader)\n",
        "print(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=os.path.join(os.getcwd(), 'data', 'animals-10', 'train'), transform=transform)\n",
        "test_data = datasets.ImageFolder(root=os.path.join(os.getcwd(), 'data', 'animals-10', 'test'), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = VGGNet16().to('cuda')\n",
        "model.apply(VGGNet16.init_params)\n",
        "\n",
        "epochs = 50\n",
        "lr = 1e-3\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_loader: DataLoader, criterion: nn.modules.loss._Loss, optimizer: optim.Optimizer, scheduler: optim.lr_scheduler.StepLR, epochs: int):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for idx, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print(f'Epoch: {epoch + 1}, Step: {idx + 1}/{len(train_loader)}, Loss: {running_loss / (idx + 1):.6f}, Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "        scheduler.step()\n",
        "                \n",
        "def test(model: nn.Module, test_loader: DataLoader, criterion: nn.modules.loss._Loss):\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        t_loss = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            pred_vals, pred_indices = torch.max(outputs, dim=1)\n",
        "            \n",
        "            t_loss += criterion(outputs, labels).item()\n",
        "            correct += pred_indices.eq(labels).sum().item()\n",
        "        \n",
        "        t_loss /= len(test_loader)\n",
        "        acc = correct / len(test_loader.dataset)\n",
        "    \n",
        "    print('Test')\n",
        "    print(f'Loss: {t_loss:.6f}, Accuracy: {acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Step: 328/655, Loss: 27.473650, Accuracy: 17.93%\n",
            "Epoch: 1, Step: 655/655, Loss: 14.860885, Accuracy: 18.30%\n",
            "Epoch: 2, Step: 328/655, Loss: 2.266070, Accuracy: 18.67%\n",
            "Epoch: 2, Step: 655/655, Loss: 2.237926, Accuracy: 18.34%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(model, train_loader, criterion, optimizer, scheduler, epochs\u001b[38;5;241m=\u001b[39mepochs)\n",
            "Cell \u001b[1;32mIn[51], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, scheduler, epochs)\u001b[0m\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 19\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(model, train_loader, criterion, optimizer, scheduler, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test(model, test_loader, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMKkQqFUUVtP"
      },
      "source": [
        "# GoogLeNet, Inception 모듈\n",
        "\n",
        "- VGGNet을 제치고 같은 해 분류 과제에서 1등을 차지\n",
        "\n",
        "- 인셉션 블록이라는 개념을 도입하여, **인셉션 네트워크(Inception Network)**라고도 불림\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/2800/0*rbWRzjKvoGt9W3Mf.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5</sub>\n",
        "\n",
        "  <br>\n",
        "\n",
        "- 특징\n",
        "  \n",
        "  - CNN 계산 용량을 최적화하는 것을 고려\n",
        "\n",
        "  - 전형적인 합성곱, 풀링 계층으로 시작하고, 이 정보는 9개의 인셉션 모듈 스택을 통과  \n",
        "    해당 모듈을 하위 네트워크라고도 함\n",
        "\n",
        "  - 각 모듈에서 입력 특징 맵은 서로 다른 계층으로 구성된 4개의 병렬 하위 블록에 전달되고, 이를 서로 다시 연결\n",
        "\n",
        "  - 모든 합성곱과 풀링 계층의 padding옵션은 \"SAME\"이며 `stride=1`,  \n",
        "    활성화 함수는 `ReLU` 사용\n",
        "\n",
        "- 기여\n",
        "\n",
        "  - 규모가 큰 블록과 병목을 보편화\n",
        "\n",
        "  - 병목 계층으로 1x1 합성곱 계층 사용\n",
        "\n",
        "  - 완전 연결 계층 대신 풀링 계층 사용\n",
        "\n",
        "  - 중간 소실로 경사 소실 문제 해결\n",
        "\n",
        "  <img src=\"https://norman3.github.io/papers/images/google_inception/f01.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://norman3.github.io/papers/docs/google_inception.html</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P91oc7YFauuk"
      },
      "source": [
        "# ResNet - 잔차 네트워크\n",
        "\n",
        "- 네트워크의 깊이가 깊어질수록 경사가 소실되거나 폭발하는 문제를 해결하고자 함\n",
        "\n",
        "- 병목 합성곱 계층을 추가하거나 크기가 작은 커널을 사용\n",
        "\n",
        "- 152개의 훈련가능한 계층을 수직으로 연결하여 구성\n",
        "\n",
        "- 모든 합성곱과 풀링 계층에서 패딩옵셥으로 \"SAME\", stride=1 사용\n",
        "\n",
        "- 3x3 합성곱 계층 다음마다 배치 정규화 적용,  \n",
        "  1x1 합성곱 계층에는 활성화 함수가 존재하지 않음\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <img src=\"https://miro.medium.com/max/1200/1*6hF97Upuqg_LdsqWY6n_wg.png\">\n",
        "\n",
        "  <sub>[이미지 출처] https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYIix2vrbwH9"
      },
      "source": [
        "## 잔차 블록 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "0SBaUMSfaqgH"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    2개의 3x3 Convolution Layer로 이루어져 ResNet-18을 구성하는 Basic block\n",
        "    \"\"\"\n",
        "    expansion = 1\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride=1, down_sample=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.down_sample = down_sample\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        if self.down_sample is not None:\n",
        "            residual = self.down_sample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "9da3RqU_c4MZ"
      },
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "    \"\"\"\n",
        "    5개의 block으로 구성\n",
        "    \"\"\"\n",
        "    def __init__(self, block: BasicBlock, layers, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.out = nn.Linear(512 * block.expansion, n_classes)\n",
        "        \n",
        "    def _make_layer(self, block: BasicBlock, out_channels: int, blocks, stride=1):\n",
        "        down_sample = None\n",
        "        mask = stride != 1 or self.in_channels != out_channels * block.expansion\n",
        "\n",
        "        if mask:\n",
        "            down_sample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion)\n",
        "            )\n",
        "        \n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, down_sample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        \n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "        \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, (3, 3), (2, 2), (1, 1))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.out(out)\n",
        "        return out\n",
        "    \n",
        "    @staticmethod\n",
        "    def init_params(m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "b6UTGl44drbG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet18(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (down_sample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (down_sample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (down_sample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "def resnet18(n_classes=10):\n",
        "    return ResNet18(BasicBlock, [2, 2, 2, 2], n_classes)\n",
        "\n",
        "model = resnet18().to('cuda')\n",
        "model.apply(ResNet18.init_params)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.5083, 0.4799, 0.3961]) tensor([0.2634, 0.2587, 0.2701])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "transform_temp = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root='./data/animals-10/train', transform=transform_temp)\n",
        "loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "def calc_mean_std(loader: DataLoader):\n",
        "    sum, sq_sum, n_batches = 0, 0, 0\n",
        "    \n",
        "    for data, _ in loader:\n",
        "        sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        sq_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
        "        n_batches += 1\n",
        "    \n",
        "    mean = sum / n_batches\n",
        "    std = ((sq_sum / n_batches) - mean ** 2) ** .5\n",
        "    \n",
        "    return mean, std\n",
        "\n",
        "mean, std = calc_mean_std(loader)\n",
        "print(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(root=os.path.join(os.getcwd(), 'data', 'animals-10', 'train'), transform=transform)\n",
        "test_data = datasets.ImageFolder(root=os.path.join(os.getcwd(), 'data', 'animals-10', 'test'), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "lr = 1e-3\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model: nn.Module, train_loader: DataLoader, criterion: nn.modules.loss._Loss, optimizer: optim.Optimizer, scheduler: optim.lr_scheduler.StepLR, epochs: int):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for idx, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            print(f'Epoch: {epoch + 1}, Step: {idx + 1}/{len(train_loader)}, Loss: {running_loss / (idx + 1):.6f}, Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "        scheduler.step()\n",
        "                \n",
        "def test(model: nn.Module, test_loader: DataLoader, criterion: nn.modules.loss._Loss):\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        t_loss = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            pred_vals, pred_indices = torch.max(outputs, dim=1)\n",
        "            \n",
        "            t_loss += criterion(outputs, labels).item()\n",
        "            correct += pred_indices.eq(labels).sum().item()\n",
        "        \n",
        "        t_loss /= len(test_loader)\n",
        "        acc = correct / len(test_loader.dataset)\n",
        "    \n",
        "    print('Test')\n",
        "    print(f'Loss: {t_loss:.6f}, Accuracy: {acc * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Step: 1/164, Loss: 2.763364, Accuracy: 7.81%\n",
            "Epoch: 1, Step: 2/164, Loss: 3.527186, Accuracy: 12.89%\n",
            "Epoch: 1, Step: 3/164, Loss: 3.615749, Accuracy: 14.58%\n",
            "Epoch: 1, Step: 4/164, Loss: 3.427653, Accuracy: 14.45%\n",
            "Epoch: 1, Step: 5/164, Loss: 3.201532, Accuracy: 15.62%\n",
            "Epoch: 1, Step: 6/164, Loss: 3.030017, Accuracy: 17.19%\n",
            "Epoch: 1, Step: 7/164, Loss: 2.976851, Accuracy: 18.30%\n",
            "Epoch: 1, Step: 8/164, Loss: 2.883705, Accuracy: 18.36%\n",
            "Epoch: 1, Step: 9/164, Loss: 2.802215, Accuracy: 19.10%\n",
            "Epoch: 1, Step: 10/164, Loss: 2.744764, Accuracy: 20.31%\n",
            "Epoch: 1, Step: 11/164, Loss: 2.692424, Accuracy: 20.60%\n",
            "Epoch: 1, Step: 12/164, Loss: 2.640583, Accuracy: 21.16%\n",
            "Epoch: 1, Step: 13/164, Loss: 2.606119, Accuracy: 20.97%\n",
            "Epoch: 1, Step: 14/164, Loss: 2.571197, Accuracy: 21.09%\n",
            "Epoch: 1, Step: 15/164, Loss: 2.538151, Accuracy: 21.46%\n",
            "Epoch: 1, Step: 16/164, Loss: 2.504676, Accuracy: 21.58%\n",
            "Epoch: 1, Step: 17/164, Loss: 2.478631, Accuracy: 22.01%\n",
            "Epoch: 1, Step: 18/164, Loss: 2.447697, Accuracy: 22.57%\n",
            "Epoch: 1, Step: 19/164, Loss: 2.425190, Accuracy: 22.94%\n",
            "Epoch: 1, Step: 20/164, Loss: 2.406452, Accuracy: 22.93%\n",
            "Epoch: 1, Step: 21/164, Loss: 2.386197, Accuracy: 23.29%\n",
            "Epoch: 1, Step: 22/164, Loss: 2.367498, Accuracy: 23.51%\n",
            "Epoch: 1, Step: 23/164, Loss: 2.350914, Accuracy: 23.68%\n",
            "Epoch: 1, Step: 24/164, Loss: 2.327268, Accuracy: 24.32%\n",
            "Epoch: 1, Step: 25/164, Loss: 2.304424, Accuracy: 24.94%\n",
            "Epoch: 1, Step: 26/164, Loss: 2.291589, Accuracy: 25.15%\n",
            "Epoch: 1, Step: 27/164, Loss: 2.275847, Accuracy: 25.55%\n",
            "Epoch: 1, Step: 28/164, Loss: 2.261575, Accuracy: 25.86%\n",
            "Epoch: 1, Step: 29/164, Loss: 2.257641, Accuracy: 25.84%\n",
            "Epoch: 1, Step: 30/164, Loss: 2.246636, Accuracy: 26.15%\n",
            "Epoch: 1, Step: 31/164, Loss: 2.239224, Accuracy: 26.21%\n",
            "Epoch: 1, Step: 32/164, Loss: 2.228980, Accuracy: 26.39%\n",
            "Epoch: 1, Step: 33/164, Loss: 2.222427, Accuracy: 26.59%\n",
            "Epoch: 1, Step: 34/164, Loss: 2.215299, Accuracy: 26.63%\n",
            "Epoch: 1, Step: 35/164, Loss: 2.210259, Accuracy: 26.67%\n",
            "Epoch: 1, Step: 36/164, Loss: 2.200003, Accuracy: 26.89%\n",
            "Epoch: 1, Step: 37/164, Loss: 2.193702, Accuracy: 27.17%\n",
            "Epoch: 1, Step: 38/164, Loss: 2.187399, Accuracy: 27.22%\n",
            "Epoch: 1, Step: 39/164, Loss: 2.179863, Accuracy: 27.30%\n",
            "Epoch: 1, Step: 40/164, Loss: 2.176960, Accuracy: 27.32%\n",
            "Epoch: 1, Step: 41/164, Loss: 2.172324, Accuracy: 27.52%\n",
            "Epoch: 1, Step: 42/164, Loss: 2.165293, Accuracy: 27.49%\n",
            "Epoch: 1, Step: 43/164, Loss: 2.161647, Accuracy: 27.56%\n",
            "Epoch: 1, Step: 44/164, Loss: 2.155921, Accuracy: 27.61%\n",
            "Epoch: 1, Step: 45/164, Loss: 2.152851, Accuracy: 27.57%\n",
            "Epoch: 1, Step: 46/164, Loss: 2.146646, Accuracy: 27.67%\n",
            "Epoch: 1, Step: 47/164, Loss: 2.141278, Accuracy: 27.69%\n",
            "Epoch: 1, Step: 48/164, Loss: 2.138480, Accuracy: 27.80%\n",
            "Epoch: 1, Step: 49/164, Loss: 2.133933, Accuracy: 27.95%\n",
            "Epoch: 1, Step: 50/164, Loss: 2.130638, Accuracy: 28.16%\n",
            "Epoch: 1, Step: 51/164, Loss: 2.127524, Accuracy: 28.29%\n",
            "Epoch: 1, Step: 52/164, Loss: 2.123429, Accuracy: 28.43%\n",
            "Epoch: 1, Step: 53/164, Loss: 2.117121, Accuracy: 28.67%\n",
            "Epoch: 1, Step: 54/164, Loss: 2.108958, Accuracy: 28.94%\n",
            "Epoch: 1, Step: 55/164, Loss: 2.105939, Accuracy: 28.99%\n",
            "Epoch: 1, Step: 56/164, Loss: 2.105227, Accuracy: 29.06%\n",
            "Epoch: 1, Step: 57/164, Loss: 2.100612, Accuracy: 29.17%\n",
            "Epoch: 1, Step: 58/164, Loss: 2.097742, Accuracy: 29.27%\n",
            "Epoch: 1, Step: 59/164, Loss: 2.093869, Accuracy: 29.38%\n",
            "Epoch: 1, Step: 60/164, Loss: 2.091844, Accuracy: 29.52%\n",
            "Epoch: 1, Step: 61/164, Loss: 2.087500, Accuracy: 29.71%\n",
            "Epoch: 1, Step: 62/164, Loss: 2.083837, Accuracy: 29.78%\n",
            "Epoch: 1, Step: 63/164, Loss: 2.078759, Accuracy: 29.96%\n",
            "Epoch: 1, Step: 64/164, Loss: 2.076303, Accuracy: 30.07%\n",
            "Epoch: 1, Step: 65/164, Loss: 2.070952, Accuracy: 30.22%\n",
            "Epoch: 1, Step: 66/164, Loss: 2.069650, Accuracy: 30.26%\n",
            "Epoch: 1, Step: 67/164, Loss: 2.064653, Accuracy: 30.43%\n",
            "Epoch: 1, Step: 68/164, Loss: 2.060080, Accuracy: 30.56%\n",
            "Epoch: 1, Step: 69/164, Loss: 2.056216, Accuracy: 30.74%\n",
            "Epoch: 1, Step: 70/164, Loss: 2.054829, Accuracy: 30.80%\n",
            "Epoch: 1, Step: 71/164, Loss: 2.053531, Accuracy: 30.81%\n",
            "Epoch: 1, Step: 72/164, Loss: 2.049289, Accuracy: 30.86%\n",
            "Epoch: 1, Step: 73/164, Loss: 2.044880, Accuracy: 30.98%\n",
            "Epoch: 1, Step: 74/164, Loss: 2.043983, Accuracy: 30.99%\n",
            "Epoch: 1, Step: 75/164, Loss: 2.040112, Accuracy: 31.16%\n",
            "Epoch: 1, Step: 76/164, Loss: 2.037024, Accuracy: 31.20%\n",
            "Epoch: 1, Step: 77/164, Loss: 2.032898, Accuracy: 31.31%\n",
            "Epoch: 1, Step: 78/164, Loss: 2.031127, Accuracy: 31.37%\n",
            "Epoch: 1, Step: 79/164, Loss: 2.028843, Accuracy: 31.48%\n",
            "Epoch: 1, Step: 80/164, Loss: 2.025008, Accuracy: 31.50%\n",
            "Epoch: 1, Step: 81/164, Loss: 2.020722, Accuracy: 31.59%\n",
            "Epoch: 1, Step: 82/164, Loss: 2.018369, Accuracy: 31.60%\n",
            "Epoch: 1, Step: 83/164, Loss: 2.016182, Accuracy: 31.66%\n",
            "Epoch: 1, Step: 84/164, Loss: 2.012936, Accuracy: 31.73%\n",
            "Epoch: 1, Step: 85/164, Loss: 2.009736, Accuracy: 31.82%\n",
            "Epoch: 1, Step: 86/164, Loss: 2.009087, Accuracy: 31.90%\n",
            "Epoch: 1, Step: 87/164, Loss: 2.006551, Accuracy: 32.03%\n",
            "Epoch: 1, Step: 88/164, Loss: 2.003835, Accuracy: 32.07%\n",
            "Epoch: 1, Step: 89/164, Loss: 2.000874, Accuracy: 32.18%\n",
            "Epoch: 1, Step: 90/164, Loss: 1.997296, Accuracy: 32.28%\n",
            "Epoch: 1, Step: 91/164, Loss: 1.995120, Accuracy: 32.28%\n",
            "Epoch: 1, Step: 92/164, Loss: 1.992423, Accuracy: 32.32%\n",
            "Epoch: 1, Step: 93/164, Loss: 1.989188, Accuracy: 32.37%\n",
            "Epoch: 1, Step: 94/164, Loss: 1.986085, Accuracy: 32.39%\n",
            "Epoch: 1, Step: 95/164, Loss: 1.984194, Accuracy: 32.40%\n",
            "Epoch: 1, Step: 96/164, Loss: 1.984261, Accuracy: 32.37%\n",
            "Epoch: 1, Step: 97/164, Loss: 1.982235, Accuracy: 32.39%\n",
            "Epoch: 1, Step: 98/164, Loss: 1.980702, Accuracy: 32.46%\n",
            "Epoch: 1, Step: 99/164, Loss: 1.978241, Accuracy: 32.55%\n",
            "Epoch: 1, Step: 100/164, Loss: 1.975812, Accuracy: 32.61%\n",
            "Epoch: 1, Step: 101/164, Loss: 1.972004, Accuracy: 32.76%\n",
            "Epoch: 1, Step: 102/164, Loss: 1.970837, Accuracy: 32.80%\n",
            "Epoch: 1, Step: 103/164, Loss: 1.971890, Accuracy: 32.82%\n",
            "Epoch: 1, Step: 104/164, Loss: 1.970132, Accuracy: 32.90%\n",
            "Epoch: 1, Step: 105/164, Loss: 1.966684, Accuracy: 32.98%\n",
            "Epoch: 1, Step: 106/164, Loss: 1.964075, Accuracy: 33.08%\n",
            "Epoch: 1, Step: 107/164, Loss: 1.961198, Accuracy: 33.20%\n",
            "Epoch: 1, Step: 108/164, Loss: 1.958840, Accuracy: 33.23%\n",
            "Epoch: 1, Step: 109/164, Loss: 1.957159, Accuracy: 33.30%\n",
            "Epoch: 1, Step: 110/164, Loss: 1.954431, Accuracy: 33.37%\n",
            "Epoch: 1, Step: 111/164, Loss: 1.952140, Accuracy: 33.42%\n",
            "Epoch: 1, Step: 112/164, Loss: 1.948809, Accuracy: 33.52%\n",
            "Epoch: 1, Step: 113/164, Loss: 1.946789, Accuracy: 33.57%\n",
            "Epoch: 1, Step: 114/164, Loss: 1.944348, Accuracy: 33.63%\n",
            "Epoch: 1, Step: 115/164, Loss: 1.942582, Accuracy: 33.68%\n",
            "Epoch: 1, Step: 116/164, Loss: 1.940528, Accuracy: 33.74%\n",
            "Epoch: 1, Step: 117/164, Loss: 1.938432, Accuracy: 33.82%\n",
            "Epoch: 1, Step: 118/164, Loss: 1.936426, Accuracy: 33.92%\n",
            "Epoch: 1, Step: 119/164, Loss: 1.935355, Accuracy: 33.95%\n",
            "Epoch: 1, Step: 120/164, Loss: 1.932668, Accuracy: 34.04%\n",
            "Epoch: 1, Step: 121/164, Loss: 1.930874, Accuracy: 34.14%\n",
            "Epoch: 1, Step: 122/164, Loss: 1.930708, Accuracy: 34.14%\n",
            "Epoch: 1, Step: 123/164, Loss: 1.928952, Accuracy: 34.21%\n",
            "Epoch: 1, Step: 124/164, Loss: 1.926769, Accuracy: 34.27%\n",
            "Epoch: 1, Step: 125/164, Loss: 1.925365, Accuracy: 34.39%\n",
            "Epoch: 1, Step: 126/164, Loss: 1.923869, Accuracy: 34.44%\n",
            "Epoch: 1, Step: 127/164, Loss: 1.921743, Accuracy: 34.51%\n",
            "Epoch: 1, Step: 128/164, Loss: 1.919973, Accuracy: 34.55%\n",
            "Epoch: 1, Step: 129/164, Loss: 1.918734, Accuracy: 34.59%\n",
            "Epoch: 1, Step: 130/164, Loss: 1.917079, Accuracy: 34.66%\n",
            "Epoch: 1, Step: 131/164, Loss: 1.914411, Accuracy: 34.74%\n",
            "Epoch: 1, Step: 132/164, Loss: 1.913389, Accuracy: 34.75%\n",
            "Epoch: 1, Step: 133/164, Loss: 1.911703, Accuracy: 34.79%\n",
            "Epoch: 1, Step: 134/164, Loss: 1.910697, Accuracy: 34.79%\n",
            "Epoch: 1, Step: 135/164, Loss: 1.908275, Accuracy: 34.83%\n",
            "Epoch: 1, Step: 136/164, Loss: 1.906463, Accuracy: 34.90%\n",
            "Epoch: 1, Step: 137/164, Loss: 1.905092, Accuracy: 34.92%\n",
            "Epoch: 1, Step: 138/164, Loss: 1.902628, Accuracy: 34.99%\n",
            "Epoch: 1, Step: 139/164, Loss: 1.901801, Accuracy: 35.03%\n",
            "Epoch: 1, Step: 140/164, Loss: 1.901500, Accuracy: 35.05%\n",
            "Epoch: 1, Step: 141/164, Loss: 1.899747, Accuracy: 35.10%\n",
            "Epoch: 1, Step: 142/164, Loss: 1.898779, Accuracy: 35.08%\n",
            "Epoch: 1, Step: 143/164, Loss: 1.896140, Accuracy: 35.17%\n",
            "Epoch: 1, Step: 144/164, Loss: 1.894211, Accuracy: 35.24%\n",
            "Epoch: 1, Step: 145/164, Loss: 1.894633, Accuracy: 35.23%\n",
            "Epoch: 1, Step: 146/164, Loss: 1.892500, Accuracy: 35.33%\n",
            "Epoch: 1, Step: 147/164, Loss: 1.890627, Accuracy: 35.41%\n",
            "Epoch: 1, Step: 148/164, Loss: 1.889212, Accuracy: 35.44%\n",
            "Epoch: 1, Step: 149/164, Loss: 1.888892, Accuracy: 35.43%\n",
            "Epoch: 1, Step: 150/164, Loss: 1.887617, Accuracy: 35.48%\n",
            "Epoch: 1, Step: 151/164, Loss: 1.887022, Accuracy: 35.47%\n",
            "Epoch: 1, Step: 152/164, Loss: 1.885736, Accuracy: 35.48%\n",
            "Epoch: 1, Step: 153/164, Loss: 1.884132, Accuracy: 35.56%\n",
            "Epoch: 1, Step: 154/164, Loss: 1.883652, Accuracy: 35.57%\n",
            "Epoch: 1, Step: 155/164, Loss: 1.881828, Accuracy: 35.62%\n",
            "Epoch: 1, Step: 156/164, Loss: 1.880671, Accuracy: 35.66%\n",
            "Epoch: 1, Step: 157/164, Loss: 1.878772, Accuracy: 35.73%\n",
            "Epoch: 1, Step: 158/164, Loss: 1.876762, Accuracy: 35.80%\n",
            "Epoch: 1, Step: 159/164, Loss: 1.874664, Accuracy: 35.86%\n",
            "Epoch: 1, Step: 160/164, Loss: 1.873458, Accuracy: 35.90%\n",
            "Epoch: 1, Step: 161/164, Loss: 1.871501, Accuracy: 35.98%\n",
            "Epoch: 1, Step: 162/164, Loss: 1.869924, Accuracy: 36.02%\n",
            "Epoch: 1, Step: 163/164, Loss: 1.868361, Accuracy: 36.08%\n",
            "Epoch: 1, Step: 164/164, Loss: 1.866191, Accuracy: 36.12%\n",
            "Epoch: 2, Step: 1/164, Loss: 1.404626, Accuracy: 46.88%\n",
            "Epoch: 2, Step: 2/164, Loss: 1.485313, Accuracy: 48.83%\n",
            "Epoch: 2, Step: 3/164, Loss: 1.546198, Accuracy: 48.18%\n",
            "Epoch: 2, Step: 4/164, Loss: 1.568679, Accuracy: 47.85%\n",
            "Epoch: 2, Step: 5/164, Loss: 1.582379, Accuracy: 47.03%\n",
            "Epoch: 2, Step: 6/164, Loss: 1.568889, Accuracy: 47.92%\n",
            "Epoch: 2, Step: 7/164, Loss: 1.597113, Accuracy: 46.88%\n",
            "Epoch: 2, Step: 8/164, Loss: 1.606247, Accuracy: 46.48%\n",
            "Epoch: 2, Step: 9/164, Loss: 1.610510, Accuracy: 46.61%\n",
            "Epoch: 2, Step: 10/164, Loss: 1.613603, Accuracy: 45.55%\n",
            "Epoch: 2, Step: 11/164, Loss: 1.609772, Accuracy: 45.67%\n",
            "Epoch: 2, Step: 12/164, Loss: 1.613729, Accuracy: 45.51%\n",
            "Epoch: 2, Step: 13/164, Loss: 1.607692, Accuracy: 45.91%\n",
            "Epoch: 2, Step: 14/164, Loss: 1.613632, Accuracy: 45.65%\n",
            "Epoch: 2, Step: 15/164, Loss: 1.614466, Accuracy: 45.36%\n",
            "Epoch: 2, Step: 16/164, Loss: 1.622831, Accuracy: 45.51%\n",
            "Epoch: 2, Step: 17/164, Loss: 1.616747, Accuracy: 45.77%\n",
            "Epoch: 2, Step: 18/164, Loss: 1.604074, Accuracy: 46.05%\n",
            "Epoch: 2, Step: 19/164, Loss: 1.605204, Accuracy: 45.76%\n",
            "Epoch: 2, Step: 20/164, Loss: 1.603332, Accuracy: 45.90%\n",
            "Epoch: 2, Step: 21/164, Loss: 1.595587, Accuracy: 45.87%\n",
            "Epoch: 2, Step: 22/164, Loss: 1.601873, Accuracy: 45.70%\n",
            "Epoch: 2, Step: 23/164, Loss: 1.602757, Accuracy: 45.89%\n",
            "Epoch: 2, Step: 24/164, Loss: 1.603508, Accuracy: 45.77%\n",
            "Epoch: 2, Step: 25/164, Loss: 1.606763, Accuracy: 45.56%\n",
            "Epoch: 2, Step: 26/164, Loss: 1.605237, Accuracy: 45.58%\n",
            "Epoch: 2, Step: 27/164, Loss: 1.598402, Accuracy: 45.95%\n",
            "Epoch: 2, Step: 28/164, Loss: 1.596500, Accuracy: 45.93%\n",
            "Epoch: 2, Step: 29/164, Loss: 1.600423, Accuracy: 45.72%\n",
            "Epoch: 2, Step: 30/164, Loss: 1.600753, Accuracy: 45.73%\n",
            "Epoch: 2, Step: 31/164, Loss: 1.601063, Accuracy: 45.67%\n",
            "Epoch: 2, Step: 32/164, Loss: 1.600509, Accuracy: 45.75%\n",
            "Epoch: 2, Step: 33/164, Loss: 1.599699, Accuracy: 45.62%\n",
            "Epoch: 2, Step: 34/164, Loss: 1.600405, Accuracy: 45.59%\n",
            "Epoch: 2, Step: 35/164, Loss: 1.601316, Accuracy: 45.56%\n",
            "Epoch: 2, Step: 36/164, Loss: 1.601622, Accuracy: 45.49%\n",
            "Epoch: 2, Step: 37/164, Loss: 1.597764, Accuracy: 45.54%\n",
            "Epoch: 2, Step: 38/164, Loss: 1.599219, Accuracy: 45.48%\n",
            "Epoch: 2, Step: 39/164, Loss: 1.601833, Accuracy: 45.41%\n",
            "Epoch: 2, Step: 40/164, Loss: 1.603865, Accuracy: 45.37%\n",
            "Epoch: 2, Step: 41/164, Loss: 1.598410, Accuracy: 45.50%\n",
            "Epoch: 2, Step: 42/164, Loss: 1.599063, Accuracy: 45.61%\n",
            "Epoch: 2, Step: 43/164, Loss: 1.596395, Accuracy: 45.57%\n",
            "Epoch: 2, Step: 44/164, Loss: 1.593535, Accuracy: 45.79%\n",
            "Epoch: 2, Step: 45/164, Loss: 1.591043, Accuracy: 45.83%\n",
            "Epoch: 2, Step: 46/164, Loss: 1.589058, Accuracy: 45.94%\n",
            "Epoch: 2, Step: 47/164, Loss: 1.591814, Accuracy: 45.83%\n",
            "Epoch: 2, Step: 48/164, Loss: 1.588728, Accuracy: 45.90%\n",
            "Epoch: 2, Step: 49/164, Loss: 1.587212, Accuracy: 45.90%\n",
            "Epoch: 2, Step: 50/164, Loss: 1.588403, Accuracy: 45.91%\n",
            "Epoch: 2, Step: 51/164, Loss: 1.586285, Accuracy: 45.82%\n",
            "Epoch: 2, Step: 52/164, Loss: 1.588565, Accuracy: 45.76%\n",
            "Epoch: 2, Step: 53/164, Loss: 1.586074, Accuracy: 45.81%\n",
            "Epoch: 2, Step: 54/164, Loss: 1.585458, Accuracy: 45.85%\n",
            "Epoch: 2, Step: 55/164, Loss: 1.585566, Accuracy: 45.84%\n",
            "Epoch: 2, Step: 56/164, Loss: 1.583549, Accuracy: 45.86%\n",
            "Epoch: 2, Step: 57/164, Loss: 1.582931, Accuracy: 45.86%\n",
            "Epoch: 2, Step: 58/164, Loss: 1.581926, Accuracy: 45.86%\n",
            "Epoch: 2, Step: 59/164, Loss: 1.581080, Accuracy: 45.86%\n",
            "Epoch: 2, Step: 60/164, Loss: 1.579470, Accuracy: 46.00%\n",
            "Epoch: 2, Step: 61/164, Loss: 1.579050, Accuracy: 46.03%\n",
            "Epoch: 2, Step: 62/164, Loss: 1.577535, Accuracy: 46.08%\n",
            "Epoch: 2, Step: 63/164, Loss: 1.577100, Accuracy: 46.12%\n",
            "Epoch: 2, Step: 64/164, Loss: 1.573640, Accuracy: 46.23%\n",
            "Epoch: 2, Step: 65/164, Loss: 1.571265, Accuracy: 46.30%\n",
            "Epoch: 2, Step: 66/164, Loss: 1.570608, Accuracy: 46.31%\n",
            "Epoch: 2, Step: 67/164, Loss: 1.570863, Accuracy: 46.29%\n",
            "Epoch: 2, Step: 68/164, Loss: 1.570627, Accuracy: 46.31%\n",
            "Epoch: 2, Step: 69/164, Loss: 1.568888, Accuracy: 46.31%\n",
            "Epoch: 2, Step: 70/164, Loss: 1.568760, Accuracy: 46.26%\n",
            "Epoch: 2, Step: 71/164, Loss: 1.569431, Accuracy: 46.17%\n",
            "Epoch: 2, Step: 72/164, Loss: 1.567159, Accuracy: 46.18%\n",
            "Epoch: 2, Step: 73/164, Loss: 1.569090, Accuracy: 46.21%\n",
            "Epoch: 2, Step: 74/164, Loss: 1.567453, Accuracy: 46.37%\n",
            "Epoch: 2, Step: 75/164, Loss: 1.566105, Accuracy: 46.46%\n",
            "Epoch: 2, Step: 76/164, Loss: 1.564152, Accuracy: 46.62%\n",
            "Epoch: 2, Step: 77/164, Loss: 1.559925, Accuracy: 46.77%\n",
            "Epoch: 2, Step: 78/164, Loss: 1.560297, Accuracy: 46.76%\n",
            "Epoch: 2, Step: 79/164, Loss: 1.560982, Accuracy: 46.75%\n",
            "Epoch: 2, Step: 80/164, Loss: 1.559140, Accuracy: 46.81%\n",
            "Epoch: 2, Step: 81/164, Loss: 1.560188, Accuracy: 46.75%\n",
            "Epoch: 2, Step: 82/164, Loss: 1.560537, Accuracy: 46.75%\n",
            "Epoch: 2, Step: 83/164, Loss: 1.559688, Accuracy: 46.79%\n",
            "Epoch: 2, Step: 84/164, Loss: 1.556637, Accuracy: 46.94%\n",
            "Epoch: 2, Step: 85/164, Loss: 1.554296, Accuracy: 47.02%\n",
            "Epoch: 2, Step: 86/164, Loss: 1.554643, Accuracy: 46.99%\n",
            "Epoch: 2, Step: 87/164, Loss: 1.553496, Accuracy: 47.05%\n",
            "Epoch: 2, Step: 88/164, Loss: 1.552528, Accuracy: 47.02%\n",
            "Epoch: 2, Step: 89/164, Loss: 1.549918, Accuracy: 47.14%\n",
            "Epoch: 2, Step: 90/164, Loss: 1.548046, Accuracy: 47.20%\n",
            "Epoch: 2, Step: 91/164, Loss: 1.547134, Accuracy: 47.23%\n",
            "Epoch: 2, Step: 92/164, Loss: 1.545344, Accuracy: 47.28%\n",
            "Epoch: 2, Step: 93/164, Loss: 1.546605, Accuracy: 47.21%\n",
            "Epoch: 2, Step: 94/164, Loss: 1.548003, Accuracy: 47.12%\n",
            "Epoch: 2, Step: 95/164, Loss: 1.548457, Accuracy: 47.17%\n",
            "Epoch: 2, Step: 96/164, Loss: 1.547640, Accuracy: 47.22%\n",
            "Epoch: 2, Step: 97/164, Loss: 1.547314, Accuracy: 47.25%\n",
            "Epoch: 2, Step: 98/164, Loss: 1.546633, Accuracy: 47.20%\n",
            "Epoch: 2, Step: 99/164, Loss: 1.546257, Accuracy: 47.22%\n",
            "Epoch: 2, Step: 100/164, Loss: 1.547220, Accuracy: 47.16%\n",
            "Epoch: 2, Step: 101/164, Loss: 1.547309, Accuracy: 47.13%\n",
            "Epoch: 2, Step: 102/164, Loss: 1.544764, Accuracy: 47.29%\n",
            "Epoch: 2, Step: 103/164, Loss: 1.543898, Accuracy: 47.31%\n",
            "Epoch: 2, Step: 104/164, Loss: 1.543314, Accuracy: 47.30%\n",
            "Epoch: 2, Step: 105/164, Loss: 1.542461, Accuracy: 47.33%\n",
            "Epoch: 2, Step: 106/164, Loss: 1.540459, Accuracy: 47.38%\n",
            "Epoch: 2, Step: 107/164, Loss: 1.540097, Accuracy: 47.40%\n",
            "Epoch: 2, Step: 108/164, Loss: 1.538745, Accuracy: 47.41%\n",
            "Epoch: 2, Step: 109/164, Loss: 1.539808, Accuracy: 47.43%\n",
            "Epoch: 2, Step: 110/164, Loss: 1.539343, Accuracy: 47.45%\n",
            "Epoch: 2, Step: 111/164, Loss: 1.536993, Accuracy: 47.48%\n",
            "Epoch: 2, Step: 112/164, Loss: 1.535938, Accuracy: 47.51%\n",
            "Epoch: 2, Step: 113/164, Loss: 1.535343, Accuracy: 47.50%\n",
            "Epoch: 2, Step: 114/164, Loss: 1.534677, Accuracy: 47.49%\n",
            "Epoch: 2, Step: 115/164, Loss: 1.534133, Accuracy: 47.60%\n",
            "Epoch: 2, Step: 116/164, Loss: 1.535613, Accuracy: 47.58%\n",
            "Epoch: 2, Step: 117/164, Loss: 1.533717, Accuracy: 47.68%\n",
            "Epoch: 2, Step: 118/164, Loss: 1.533571, Accuracy: 47.65%\n",
            "Epoch: 2, Step: 119/164, Loss: 1.532610, Accuracy: 47.68%\n",
            "Epoch: 2, Step: 120/164, Loss: 1.531644, Accuracy: 47.68%\n",
            "Epoch: 2, Step: 121/164, Loss: 1.530555, Accuracy: 47.73%\n",
            "Epoch: 2, Step: 122/164, Loss: 1.529845, Accuracy: 47.74%\n",
            "Epoch: 2, Step: 123/164, Loss: 1.528423, Accuracy: 47.78%\n",
            "Epoch: 2, Step: 124/164, Loss: 1.527771, Accuracy: 47.81%\n",
            "Epoch: 2, Step: 125/164, Loss: 1.527288, Accuracy: 47.80%\n",
            "Epoch: 2, Step: 126/164, Loss: 1.527131, Accuracy: 47.82%\n",
            "Epoch: 2, Step: 127/164, Loss: 1.526326, Accuracy: 47.82%\n",
            "Epoch: 2, Step: 128/164, Loss: 1.524147, Accuracy: 47.91%\n",
            "Epoch: 2, Step: 129/164, Loss: 1.523166, Accuracy: 47.93%\n",
            "Epoch: 2, Step: 130/164, Loss: 1.522651, Accuracy: 47.96%\n",
            "Epoch: 2, Step: 131/164, Loss: 1.522970, Accuracy: 47.92%\n",
            "Epoch: 2, Step: 132/164, Loss: 1.522376, Accuracy: 47.93%\n",
            "Epoch: 2, Step: 133/164, Loss: 1.522336, Accuracy: 47.90%\n",
            "Epoch: 2, Step: 134/164, Loss: 1.522275, Accuracy: 47.95%\n",
            "Epoch: 2, Step: 135/164, Loss: 1.521321, Accuracy: 47.99%\n",
            "Epoch: 2, Step: 136/164, Loss: 1.520529, Accuracy: 47.98%\n",
            "Epoch: 2, Step: 137/164, Loss: 1.520806, Accuracy: 47.98%\n",
            "Epoch: 2, Step: 138/164, Loss: 1.521153, Accuracy: 47.97%\n",
            "Epoch: 2, Step: 139/164, Loss: 1.520366, Accuracy: 48.01%\n",
            "Epoch: 2, Step: 140/164, Loss: 1.520657, Accuracy: 48.00%\n",
            "Epoch: 2, Step: 141/164, Loss: 1.520520, Accuracy: 48.03%\n",
            "Epoch: 2, Step: 142/164, Loss: 1.521788, Accuracy: 47.98%\n",
            "Epoch: 2, Step: 143/164, Loss: 1.521438, Accuracy: 47.97%\n",
            "Epoch: 2, Step: 144/164, Loss: 1.522633, Accuracy: 47.91%\n",
            "Epoch: 2, Step: 145/164, Loss: 1.521150, Accuracy: 47.95%\n",
            "Epoch: 2, Step: 146/164, Loss: 1.521113, Accuracy: 47.97%\n",
            "Epoch: 2, Step: 147/164, Loss: 1.520733, Accuracy: 47.99%\n",
            "Epoch: 2, Step: 148/164, Loss: 1.520978, Accuracy: 47.98%\n",
            "Epoch: 2, Step: 149/164, Loss: 1.520298, Accuracy: 48.00%\n",
            "Epoch: 2, Step: 150/164, Loss: 1.518879, Accuracy: 48.08%\n",
            "Epoch: 2, Step: 151/164, Loss: 1.518514, Accuracy: 48.10%\n",
            "Epoch: 2, Step: 152/164, Loss: 1.517988, Accuracy: 48.11%\n",
            "Epoch: 2, Step: 153/164, Loss: 1.519429, Accuracy: 48.09%\n",
            "Epoch: 2, Step: 154/164, Loss: 1.519484, Accuracy: 48.08%\n",
            "Epoch: 2, Step: 155/164, Loss: 1.519247, Accuracy: 48.08%\n",
            "Epoch: 2, Step: 156/164, Loss: 1.519173, Accuracy: 48.10%\n",
            "Epoch: 2, Step: 157/164, Loss: 1.519432, Accuracy: 48.09%\n",
            "Epoch: 2, Step: 158/164, Loss: 1.518499, Accuracy: 48.11%\n",
            "Epoch: 2, Step: 159/164, Loss: 1.517424, Accuracy: 48.18%\n",
            "Epoch: 2, Step: 160/164, Loss: 1.516677, Accuracy: 48.21%\n",
            "Epoch: 2, Step: 161/164, Loss: 1.515972, Accuracy: 48.25%\n",
            "Epoch: 2, Step: 162/164, Loss: 1.516261, Accuracy: 48.24%\n",
            "Epoch: 2, Step: 163/164, Loss: 1.516679, Accuracy: 48.25%\n",
            "Epoch: 2, Step: 164/164, Loss: 1.514559, Accuracy: 48.29%\n",
            "Epoch: 3, Step: 1/164, Loss: 1.592035, Accuracy: 51.56%\n",
            "Epoch: 3, Step: 2/164, Loss: 1.472334, Accuracy: 55.47%\n",
            "Epoch: 3, Step: 3/164, Loss: 1.456445, Accuracy: 54.17%\n",
            "Epoch: 3, Step: 4/164, Loss: 1.435110, Accuracy: 53.12%\n",
            "Epoch: 3, Step: 5/164, Loss: 1.437551, Accuracy: 52.81%\n",
            "Epoch: 3, Step: 6/164, Loss: 1.434899, Accuracy: 52.60%\n",
            "Epoch: 3, Step: 7/164, Loss: 1.427840, Accuracy: 52.79%\n",
            "Epoch: 3, Step: 8/164, Loss: 1.448535, Accuracy: 52.05%\n",
            "Epoch: 3, Step: 9/164, Loss: 1.436653, Accuracy: 52.69%\n",
            "Epoch: 3, Step: 10/164, Loss: 1.424777, Accuracy: 53.12%\n",
            "Epoch: 3, Step: 11/164, Loss: 1.428432, Accuracy: 52.34%\n",
            "Epoch: 3, Step: 12/164, Loss: 1.420869, Accuracy: 52.60%\n",
            "Epoch: 3, Step: 13/164, Loss: 1.417192, Accuracy: 52.28%\n",
            "Epoch: 3, Step: 14/164, Loss: 1.426336, Accuracy: 51.84%\n",
            "Epoch: 3, Step: 15/164, Loss: 1.417736, Accuracy: 52.29%\n",
            "Epoch: 3, Step: 16/164, Loss: 1.427298, Accuracy: 52.10%\n",
            "Epoch: 3, Step: 17/164, Loss: 1.422017, Accuracy: 52.07%\n",
            "Epoch: 3, Step: 18/164, Loss: 1.425684, Accuracy: 51.95%\n",
            "Epoch: 3, Step: 19/164, Loss: 1.435538, Accuracy: 51.40%\n",
            "Epoch: 3, Step: 20/164, Loss: 1.433449, Accuracy: 51.37%\n",
            "Epoch: 3, Step: 21/164, Loss: 1.432055, Accuracy: 51.49%\n",
            "Epoch: 3, Step: 22/164, Loss: 1.420486, Accuracy: 52.06%\n",
            "Epoch: 3, Step: 23/164, Loss: 1.416021, Accuracy: 52.21%\n",
            "Epoch: 3, Step: 24/164, Loss: 1.407699, Accuracy: 52.41%\n",
            "Epoch: 3, Step: 25/164, Loss: 1.400139, Accuracy: 52.69%\n",
            "Epoch: 3, Step: 26/164, Loss: 1.404322, Accuracy: 52.58%\n",
            "Epoch: 3, Step: 27/164, Loss: 1.405171, Accuracy: 52.52%\n",
            "Epoch: 3, Step: 28/164, Loss: 1.408684, Accuracy: 52.40%\n",
            "Epoch: 3, Step: 29/164, Loss: 1.407018, Accuracy: 52.45%\n",
            "Epoch: 3, Step: 30/164, Loss: 1.401162, Accuracy: 52.58%\n",
            "Epoch: 3, Step: 31/164, Loss: 1.396085, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 32/164, Loss: 1.394723, Accuracy: 52.76%\n",
            "Epoch: 3, Step: 33/164, Loss: 1.401953, Accuracy: 52.34%\n",
            "Epoch: 3, Step: 34/164, Loss: 1.400857, Accuracy: 52.44%\n",
            "Epoch: 3, Step: 35/164, Loss: 1.396338, Accuracy: 52.57%\n",
            "Epoch: 3, Step: 36/164, Loss: 1.398523, Accuracy: 52.34%\n",
            "Epoch: 3, Step: 37/164, Loss: 1.398924, Accuracy: 52.32%\n",
            "Epoch: 3, Step: 38/164, Loss: 1.393358, Accuracy: 52.51%\n",
            "Epoch: 3, Step: 39/164, Loss: 1.391290, Accuracy: 52.74%\n",
            "Epoch: 3, Step: 40/164, Loss: 1.395104, Accuracy: 52.60%\n",
            "Epoch: 3, Step: 41/164, Loss: 1.397849, Accuracy: 52.52%\n",
            "Epoch: 3, Step: 42/164, Loss: 1.392601, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 43/164, Loss: 1.389254, Accuracy: 52.78%\n",
            "Epoch: 3, Step: 44/164, Loss: 1.391998, Accuracy: 52.68%\n",
            "Epoch: 3, Step: 45/164, Loss: 1.393063, Accuracy: 52.64%\n",
            "Epoch: 3, Step: 46/164, Loss: 1.391244, Accuracy: 52.63%\n",
            "Epoch: 3, Step: 47/164, Loss: 1.388729, Accuracy: 52.68%\n",
            "Epoch: 3, Step: 48/164, Loss: 1.385913, Accuracy: 52.86%\n",
            "Epoch: 3, Step: 49/164, Loss: 1.385874, Accuracy: 52.69%\n",
            "Epoch: 3, Step: 50/164, Loss: 1.387686, Accuracy: 52.62%\n",
            "Epoch: 3, Step: 51/164, Loss: 1.387505, Accuracy: 52.59%\n",
            "Epoch: 3, Step: 52/164, Loss: 1.387146, Accuracy: 52.63%\n",
            "Epoch: 3, Step: 53/164, Loss: 1.385334, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 54/164, Loss: 1.384329, Accuracy: 52.58%\n",
            "Epoch: 3, Step: 55/164, Loss: 1.384516, Accuracy: 52.56%\n",
            "Epoch: 3, Step: 56/164, Loss: 1.382750, Accuracy: 52.59%\n",
            "Epoch: 3, Step: 57/164, Loss: 1.384998, Accuracy: 52.55%\n",
            "Epoch: 3, Step: 58/164, Loss: 1.387450, Accuracy: 52.45%\n",
            "Epoch: 3, Step: 59/164, Loss: 1.386975, Accuracy: 52.50%\n",
            "Epoch: 3, Step: 60/164, Loss: 1.385676, Accuracy: 52.50%\n",
            "Epoch: 3, Step: 61/164, Loss: 1.386311, Accuracy: 52.51%\n",
            "Epoch: 3, Step: 62/164, Loss: 1.387566, Accuracy: 52.57%\n",
            "Epoch: 3, Step: 63/164, Loss: 1.389789, Accuracy: 52.43%\n",
            "Epoch: 3, Step: 64/164, Loss: 1.388037, Accuracy: 52.49%\n",
            "Epoch: 3, Step: 65/164, Loss: 1.385764, Accuracy: 52.58%\n",
            "Epoch: 3, Step: 66/164, Loss: 1.385669, Accuracy: 52.54%\n",
            "Epoch: 3, Step: 67/164, Loss: 1.384357, Accuracy: 52.62%\n",
            "Epoch: 3, Step: 68/164, Loss: 1.385381, Accuracy: 52.61%\n",
            "Epoch: 3, Step: 69/164, Loss: 1.384873, Accuracy: 52.58%\n",
            "Epoch: 3, Step: 70/164, Loss: 1.382677, Accuracy: 52.71%\n",
            "Epoch: 3, Step: 71/164, Loss: 1.382801, Accuracy: 52.71%\n",
            "Epoch: 3, Step: 72/164, Loss: 1.383007, Accuracy: 52.67%\n",
            "Epoch: 3, Step: 73/164, Loss: 1.381756, Accuracy: 52.73%\n",
            "Epoch: 3, Step: 74/164, Loss: 1.382305, Accuracy: 52.68%\n",
            "Epoch: 3, Step: 75/164, Loss: 1.383167, Accuracy: 52.67%\n",
            "Epoch: 3, Step: 76/164, Loss: 1.383893, Accuracy: 52.65%\n",
            "Epoch: 3, Step: 77/164, Loss: 1.383673, Accuracy: 52.68%\n",
            "Epoch: 3, Step: 78/164, Loss: 1.383625, Accuracy: 52.71%\n",
            "Epoch: 3, Step: 79/164, Loss: 1.383903, Accuracy: 52.74%\n",
            "Epoch: 3, Step: 80/164, Loss: 1.385225, Accuracy: 52.72%\n",
            "Epoch: 3, Step: 81/164, Loss: 1.384999, Accuracy: 52.72%\n",
            "Epoch: 3, Step: 82/164, Loss: 1.385662, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 83/164, Loss: 1.386066, Accuracy: 52.64%\n",
            "Epoch: 3, Step: 84/164, Loss: 1.384592, Accuracy: 52.72%\n",
            "Epoch: 3, Step: 85/164, Loss: 1.384303, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 86/164, Loss: 1.386485, Accuracy: 52.73%\n",
            "Epoch: 3, Step: 87/164, Loss: 1.385992, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 88/164, Loss: 1.384644, Accuracy: 52.81%\n",
            "Epoch: 3, Step: 89/164, Loss: 1.384044, Accuracy: 52.77%\n",
            "Epoch: 3, Step: 90/164, Loss: 1.383927, Accuracy: 52.78%\n",
            "Epoch: 3, Step: 91/164, Loss: 1.383541, Accuracy: 52.78%\n",
            "Epoch: 3, Step: 92/164, Loss: 1.382847, Accuracy: 52.84%\n",
            "Epoch: 3, Step: 93/164, Loss: 1.381926, Accuracy: 52.86%\n",
            "Epoch: 3, Step: 94/164, Loss: 1.383996, Accuracy: 52.78%\n",
            "Epoch: 3, Step: 95/164, Loss: 1.384202, Accuracy: 52.74%\n",
            "Epoch: 3, Step: 96/164, Loss: 1.385139, Accuracy: 52.64%\n",
            "Epoch: 3, Step: 97/164, Loss: 1.384515, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 98/164, Loss: 1.383466, Accuracy: 52.69%\n",
            "Epoch: 3, Step: 99/164, Loss: 1.381870, Accuracy: 52.71%\n",
            "Epoch: 3, Step: 100/164, Loss: 1.382739, Accuracy: 52.70%\n",
            "Epoch: 3, Step: 101/164, Loss: 1.381599, Accuracy: 52.75%\n",
            "Epoch: 3, Step: 102/164, Loss: 1.381451, Accuracy: 52.77%\n",
            "Epoch: 3, Step: 103/164, Loss: 1.378623, Accuracy: 52.89%\n",
            "Epoch: 3, Step: 104/164, Loss: 1.379239, Accuracy: 52.87%\n",
            "Epoch: 3, Step: 105/164, Loss: 1.377838, Accuracy: 52.90%\n",
            "Epoch: 3, Step: 106/164, Loss: 1.377147, Accuracy: 52.85%\n",
            "Epoch: 3, Step: 107/164, Loss: 1.376397, Accuracy: 52.85%\n",
            "Epoch: 3, Step: 108/164, Loss: 1.375901, Accuracy: 52.87%\n"
          ]
        }
      ],
      "source": [
        "train(model, train_loader, criterion, optimizer, scheduler, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test(model, test_loader, criterion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
