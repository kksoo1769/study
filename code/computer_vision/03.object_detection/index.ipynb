{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참조 : https://github.com/wikibook/dl-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 객체 탐지 (Object Detection)\n",
    "\n",
    "- 한 이미지에서 객체와 그 경계 상자(bounding box)를 탐지\n",
    "\n",
    "- 객체 탐지 알고리즘은 일반적으로 이미지를 입력으로 받고, 경계 상자와 객체 클래스 리스트를 출력\n",
    "\n",
    "- 경계 상자에 대해 그에 대응하는 예측 클래스와 클래스의 신뢰도(confidence)를 출력\n",
    "\n",
    "## Applications\n",
    "\n",
    "- 자율 주행 자동차에서 다른 자동차와 보행자를 찾을 때\n",
    "\n",
    "- 의료 분야에서 방사선 사진을 사용해 종양이나 위험한 조직을 찾을 때\n",
    "\n",
    "- 제조업에서 조립 로봇이 제품을 조립하거나 수리할 때\n",
    "\n",
    "- 보안 산업에서 위협을 탐지하거나 사람을 셀 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 용어 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boudning Box\n",
    "\n",
    "- 이미지에서 하나의 객체 전체를 포함하는 가장 작은 직사각형\n",
    "\n",
    "  <img src=\"https://miro.medium.com/max/850/1*KL6r494Eyfh3iYEXQA2tzg.png\">\n",
    "\n",
    "  <sub>[이미지 출처] https://medium.com/anolytics/how-bounding-box-annotation-helps-object-detection-in-machine-learning-use-cases-431d93e7b25b</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU(Intersection Over Union)\n",
    "- 실측값(Ground Truth)과 모델이 예측한 값이 얼마나 겹치는지를 나타내는 지표\n",
    "\n",
    "  <img src=\"https://pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png\" width=\"300\">\n",
    "\n",
    "- IOU가 높을수록 잘 예측한 모델\n",
    "\n",
    "  <img src=\"https://pyimagesearch.com/wp-content/uploads/2016/09/iou_examples.png\" width=\"400\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- 예시\n",
    "  <img src=\"https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_stop_sign.jpg\">\n",
    "\n",
    "  <sub>[이미지 출처] https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_iou(pred_box, gt_box):\n",
    "    x1 = np.maximum(pred_box[0], gt_box[0])\n",
    "    y1 = np.maximum(pred_box[1], gt_box[1])\n",
    "    x2 = np.maximum(pred_box[2], gt_box[2])\n",
    "    y2 = np.maximum(pred_box[3], gt_box[3])\n",
    "\n",
    "    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "\n",
    "    pred_box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "    gt_box_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "\n",
    "    union = pred_box_area + gt_box_area - intersection\n",
    "    \n",
    "    iou = intersection / union\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS(Non-Maximum Suppression, 비최댓값 억제)\n",
    "\n",
    "- 확률이 가장 높은 상자와 겹치는 상자들을 제거하는 과정\n",
    "\n",
    "- 최댓값을 갖지 않는 상자들을 제거\n",
    "\n",
    "- 과정\n",
    "\n",
    "  1. 확률 기준으로 모든 상자를 정렬하고 먼저 가장 확률이 높은 상자를 취함\n",
    "\n",
    "  2. 각 상자에 대해 다른 모든 상자와의 IOU를 계산\n",
    "\n",
    "  3. 특정 임곗값을 넘는 상자는 제거\n",
    "\n",
    "  <img src=\"https://pyimagesearch.com/wp-content/uploads/2014/10/nms_fast_03.jpg\">\n",
    "\n",
    "  <sub>[이미지 출처] https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes: np.ndarray, overlap_thresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    if boxes.dtype.kind == 'i':\n",
    "        boxes = boxes.astype('float')\n",
    "    \n",
    "    pick = []\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    while len(idxs) > 0:\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        \n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.maximum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.maximum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        idxs = np.delete(idxs, np.concatenate([last], np.where(overlap > overlap_thresh)[0]))\n",
    "    \n",
    "    return boxes[pick].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정밀도와 재현율\n",
    "\n",
    "- 일반적으로 객체 탐지 모델 평가에 사용되지는 않지만, 다른 지표를 계산하는 기본 지표 역할을 함\n",
    "\n",
    "  - `TP`\n",
    "\n",
    "    - True Positives\n",
    "\n",
    "    - 예측이 동일 클래스의 실제 상자와 일치하는지 측정\n",
    "\n",
    "  - `FP`\n",
    "\n",
    "    - False Positives\n",
    "\n",
    "    - 예측이 실제 상자와 일치하지 않는지 측정\n",
    "\n",
    "  - `FN`\n",
    "\n",
    "    - False Negatives\n",
    "\n",
    "    - 실제 분류값이 그와 일치하는 예측을 갖지 못하는지 측정\n",
    "\n",
    "<br>\n",
    "\n",
    "## $\\qquad precision = \\frac{TP}{TP \\ + \\ FP}$\n",
    "## $\\qquad recall = \\frac{TP}{TP \\ + \\ FN}$\n",
    "\n",
    "  - 모델이 안정적이지 않은 특징을 기반으로 객체 존재를 예측하면 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n",
    "\n",
    "  - 모델이 너무 엄격해서 정확한 조건을 만족할 때만 객체가 탐지된 것으로 간주하면 거짓부정(FN)이 많아져서 재현율이 낮아짐\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정밀도-재현율 곡선(precision-recall curve)\n",
    "\n",
    "- 신뢰도 임곗값마다 모델의 정밀도와 재현율을 시각화\n",
    "\n",
    "- 모든 bounding box와 함께 모델이 예측의 정확성을 얼마나 확실하는지 0 ~ 1사이의 숫자로 나타내는 신뢰도를 출력\n",
    "\n",
    "- 임계값 T에 따라 정밀도와 재현율이 달라짐\n",
    "\n",
    "  - 임곗값 T 이하의 예측은 제거함\n",
    "\n",
    "  - T가 1에 가까우면 정밀도는 높지만 재현율은 낮음  \n",
    "    놓치는 객체가 많아져서 재현율이 낮아짐. 즉, 신뢰도가 높은 예측만 유지하기때문에 정밀도는 높아짐\n",
    "\n",
    "  - T가 0에 가까우면 정밀도는 낮지만 재현율은 높음  \n",
    "    대부분의 예측을 유지하기때문에 재현율은 높아지고, 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n",
    "\n",
    "- 예를 들어, 모델이 보행자를 탐지하고 있으면 특별한 이유없이 차를 세우더라도 어떤 보행자도 놓치지 않도록 재현율을 높여야 함\n",
    "-  모델이 투자 기회를 탐지하고 있다면 일부 기회를 놓치게 되더라도 잘못된 기회에 돈을 거는 일을 피하기 위해 정밀도를 높여야 함\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Davide_Chicco/publication/321672019/figure/fig1/AS:614279602511886@1523467078452/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the.png\">\n",
    "\n",
    "<sub>[이미지 출처] https://www.researchgate.net/figure/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the_fig1_321672019</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP (Average Precision, 평균 정밀도) 와 mAP(mean Average Precision)\n",
    "\n",
    "- 곡선의 아래 영역에 해당\n",
    "\n",
    "- 항상 1x1 정사각형으로 구성되어 있음  \n",
    "  즉, 항상 0 ~ 1 사이의 값을 가짐\n",
    "\n",
    "- 단일 클래스에 대한 모델 성능 정보를 제공\n",
    "\n",
    "- 전역 점수를 얻기위해서 mAP를 사용\n",
    "\n",
    "- 예를 들어, 데이터셋이 10개의 클래스로 구성된다면 각 클래스에 대한 AP를 계산하고, 그 숫자들의 평균을 다시 구함\n",
    "\n",
    "- (참고)\n",
    "\n",
    "  - 최소 2개 이상의 객체를 탐지하는 대회인 PASCAL Visual Object Classes와 Common Objects in Context(COCO)에서 mAP가 사용됨\n",
    "\n",
    "  - COCO 데이터셋이 더 많은 클래스를 포함하고 있기 때문에 보통 Pascal VOC보다 점수가 더 낮게 나옴\n",
    "\n",
    "  - 예시\n",
    "\n",
    "    <img src=\"https://www.researchgate.net/profile/Bong_Nam_Kang/publication/328939155/figure/tbl2/AS:692891936649218@1542209719916/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev.png\">\n",
    "\n",
    "    <sub>[이미지 출처] https://www.researchgate.net/figure/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev_tbl2_328939155</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC\n",
    "\n",
    "- 2005년부터 2012년까지 진행\n",
    "\n",
    "- Object Detection 기술의 benchmark로 간주\n",
    "\n",
    "- 데이터셋에는 20개의 클래스가 존재\n",
    "\n",
    "      background\n",
    "      aeroplane\n",
    "      bicycle\n",
    "      bird\n",
    "      boat\n",
    "      bottle\n",
    "      bus\n",
    "      car\n",
    "      cat\n",
    "      chair\n",
    "      cow\n",
    "      diningtable\n",
    "      dog\n",
    "      horse\n",
    "      motorbike\n",
    "      person\n",
    "      pottedplant\n",
    "      sheep\n",
    "      sofa\n",
    "      train\n",
    "      tvmonitor\n",
    "\n",
    "- 훈련 및 검증 데이터 : 11,530개\n",
    "\n",
    "- ROI에 대한 27,450개의 Annotation이 존재\n",
    "\n",
    "- 이미지당 2.4개의 객체 존재\n",
    "\n",
    "  <img src=\"https://paperswithcode.github.io/sotabench-eval/img/pascalvoc2012.png\">\n",
    "\n",
    "  <sub>[이미지 출처] https://paperswithcode.github.io/sotabench-eval/pascalvoc/</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Dataset\n",
    "\n",
    "- Common Objects in Context\n",
    "\n",
    "- 200,000개의 이미지\n",
    "\n",
    "- 80개의 카테고리에 500,000개 이상의 객체 Annotation이 존재\n",
    "      person\n",
    "      bicycle\n",
    "      car\n",
    "      motorbike\n",
    "      aeroplane\n",
    "      bus\n",
    "      train\n",
    "      truck\n",
    "      boat\n",
    "      traffic light\n",
    "      fire hydrant\n",
    "      stop sign\n",
    "      parking meter\n",
    "      bench\n",
    "      bird\n",
    "      cat\n",
    "      dog\n",
    "      horse\n",
    "      sheep\n",
    "      cow\n",
    "      elephant\n",
    "      bear\n",
    "      zebra\n",
    "      giraffe\n",
    "      backpack\n",
    "      umbrella\n",
    "      handbag\n",
    "      tie\n",
    "      suitcase\n",
    "      frisbee\n",
    "      skis\n",
    "      snowboard\n",
    "      sports ball\n",
    "      kite\n",
    "      baseball bat\n",
    "      baseball glove\n",
    "      skateboard\n",
    "      surfboard\n",
    "      tennis racket\n",
    "      bottle\n",
    "      wine glass\n",
    "      cup\n",
    "      fork\n",
    "      knife\n",
    "      spoon\n",
    "      bowl\n",
    "      banana\n",
    "      apple\n",
    "      sandwich\n",
    "      orange\n",
    "      broccoli\n",
    "      carrot\n",
    "      hot dog\n",
    "      pizza\n",
    "      donut\n",
    "      cake\n",
    "      chair\n",
    "      sofa\n",
    "      pottedplant\n",
    "      bed\n",
    "      diningtable\n",
    "      toilet\n",
    "      tvmonitor\n",
    "      laptop\n",
    "      mouse\n",
    "      remote\n",
    "      keyboard\n",
    "      cell phone\n",
    "      microwave\n",
    "      oven\n",
    "      toaster\n",
    "      sink\n",
    "      refrigerator\n",
    "      book\n",
    "      clock\n",
    "      vase\n",
    "      scissors\n",
    "      teddy bear\n",
    "      hair drier\n",
    "      toothbrush\n",
    "- https://cocodataset.org/#home\n",
    "\n",
    "<img src=\"https://cocodataset.org/images/coco-examples.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO (You Only Look Once)\n",
    "\n",
    "- 가장 빠른 객체 검출 알고리즘 중 하나\n",
    "\n",
    "- 256x256 사이즈의 이미지\n",
    "\n",
    "- GPU 사용 시, 초당 170프레임(170**FPS**, **frames per second**),  \n",
    "  이는 파이썬, 텐서플로 기반 프레임워크가 아닌 C++로 구현된 코드 기준\n",
    "\n",
    "- 작은 크기의 물체를 탐지하는데는 어려움\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*bSLNlG7crv-p-m4LVYYk3Q.png\" width=\"600\">\n",
    "\n",
    "\n",
    "- https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "- 자세한 내용 참조 : https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Backbone\n",
    "\n",
    "- 백본 모델(backbone model) 기반\n",
    "\n",
    "- 특징 추출기(Feature Extractor)라고도 불림\n",
    "\n",
    "- YOLO는 자체 맞춤 아키텍쳐 사용\n",
    "\n",
    "- 어떤 특징 추출기 아키텍쳐를 사용했는지에 따라 성능 달라짐\n",
    "\n",
    "  <img src=\"https://www.researchgate.net/publication/335865923/figure/fig1/AS:804106595758082@1568725360777/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three.jpg\">\n",
    "\n",
    "  <sub>[이미지 출처] https://www.researchgate.net/figure/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three_fig1_335865923</sub>\n",
    "\n",
    "- 마지막 계층은 크기가 $w \\times h \\times D$인 특징 볼륨 출력\n",
    "\n",
    "- $w \\times h $는 그리드의 크기이고, $D$는 특징 볼륨 깊이\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO의 계층 출력\n",
    "\n",
    "- 마지막 계층 출력은 $w \\times h \\times M$ 행렬\n",
    "  \n",
    "  - $M = B \\times (C + 5)$\n",
    "\n",
    "    - B : 그리드 셀당 경계 상자 개수\n",
    "\n",
    "    - C : 클래스 개수\n",
    "\n",
    "  - 클래스 개수에 5를 더한 이유는 해당 값 만큼의 숫자를 예측해야함\n",
    "\n",
    "    - $t_x$, $t_y$는 경계상자의 중심 좌표를 계산\n",
    "\n",
    "    - $t_w$, $t_h$는 경계상자의 너비와 높이를 계산\n",
    "\n",
    "    - $c$는 객체가 경계 상자 안에 있다고 확신하는 신뢰도\n",
    "\n",
    "    - $p1, p2, ..., pC$는 경계상자가 클래스 1, 2, ..., C의 객체를 포함할 확률\n",
    "  \n",
    "  <br>\n",
    "\n",
    "  <img src=\"https://www.researchgate.net/profile/Thi_Le3/publication/337705605/figure/fig3/AS:831927326089217@1575358339500/Structure-of-one-output-cell-in-YOLO.ppm\">\n",
    "\n",
    "  <sub>[이미지 출처] https://www.researchgate.net/figure/Structure-of-one-output-cell-in-YOLO_fig3_337705605</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앵커 박스(Anchor Box)\n",
    "\n",
    "- YOLOv2에서 도입\n",
    "\n",
    "- 사전 정의된 상자(prior box)\n",
    "\n",
    "- 객체에 가장 근접한 앵커 박스를 맞추고 신경망을 사용해 앵커 박스의 크기를 조정하는 과정때문에 $t_x, t_y, t_w, t_h$이 필요\n",
    "\n",
    "  <img src=\"https://kr.mathworks.com/help/vision/ug/ssd_detection.png\">\n",
    "\n",
    "  <sub>[이미지 출처] https://kr.mathworks.com/help/vision/ug/getting-started-with-yolo-v2.html</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 inference 연습 : Pytorch\n",
    "\n",
    "- video inference\n",
    "- 코드 참조 : https://medium.com/python-in-plain-english/how-to-count-objects-from-a-video-with-opencv-in-python-b7f6d68cb4e6\n",
    "- https://github.com/ultralytics/yolov3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare YOLOv5-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.1.16-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.4 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 20.5/40.4 kB 320.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 40.4/40.4 kB 478.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (3.8.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (10.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (0.16.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (2.1.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n",
      "Requirement already satisfied: filelock in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kksoo\\anaconda3\\envs\\torch\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.1.16-py3-none-any.whl (715 kB)\n",
      "   ---------------------------------------- 0.0/715.4 kB ? eta -:--:--\n",
      "   -------------------------------- ------ 593.9/715.4 kB 12.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 715.4/715.4 kB 15.0 MB/s eta 0:00:00\n",
      "Installing collected packages: thop, ultralytics\n",
      "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.1.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.path as mplPath\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = 'data/traffic.mp4'\n",
    "HUB = 'ultralytics/yolov5'\n",
    "YOLO = 'yolov5n'\n",
    "\n",
    "ZONE = np.array([\n",
    "    [333, 374],\n",
    "    [403, 470],\n",
    "    [476, 655],\n",
    "    [498, 710],\n",
    "    [1237, 714],\n",
    "    [1217, 523],\n",
    "    [1139, 469],\n",
    "    [1009, 393],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center(bbox):\n",
    "    center = ((bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2)\n",
    "\n",
    "    return center\n",
    "\n",
    "def load_model():\n",
    "    model = torch.hub.load(HUB, model=YOLO, pretrained=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_bboxes(preds: object):\n",
    "    df = preds.pandas().xyxy[0]\n",
    "    df = df[(df['confidence'] >= .5) & df['name'].isin(['car', 'bus', 'truck'])]\n",
    "\n",
    "    return df[['xmin', 'ymin', 'xmax', 'ymax']].values.astype(int)\n",
    "\n",
    "def is_valid_detection(xc, yc):\n",
    "    return mplPath.Path(ZONE).contains_point((xc, yc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\kksoo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-21 Python-3.11.7 torch-2.1.2 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "def count_cars(cap: cv2.VideoCapture):\n",
    "\n",
    "    model = torch.hub.load(\"ultralytics/yolov5\", model=\"yolov5n\", pretrained=True)\n",
    "    count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        status, frame = cap.read()\n",
    "\n",
    "        if not status:\n",
    "            break\n",
    "\n",
    "        preds = model(frame)\n",
    "        bboxes = get_bboxes(preds)\n",
    "\n",
    "        detections = 0\n",
    "        for box in bboxes:\n",
    "            xc, yc = get_center(box)\n",
    "            \n",
    "            if is_valid_detection(xc, yc):\n",
    "                detections += 1\n",
    "            \n",
    "            cv2.circle(img=frame, center=(xc, yc), radius=5, color=(0,255,0), thickness=-1)\n",
    "            cv2.rectangle(img=frame, pt1=(box[0], box[1]), pt2=(box[2], box[3]), color=(255, 0, 0), thickness=1)\n",
    "\n",
    "        cv2.putText(img=frame, text=f\"Cars: {detections}\", org=(100, 100), fontFace=cv2.FONT_HERSHEY_PLAIN, fontScale=3, color=(0, 0, 0), thickness=3)\n",
    "        cv2.polylines(img=frame, pts=[ZONE], isClosed=True, color=(0, 0, 255), thickness=4)\n",
    "\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "        count += 1\n",
    "\n",
    "        if cv2.waitKey(3) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "count_cars(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git clone to get short videos\n",
    "\n",
    "- https://github.com/vindruid/yolov3-in-colab.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\kksoo/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-2-21 Python-3.11.7 torch-2.1.2 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"ultralytics/yolov5\", model=\"yolov5n\", pretrained=True)\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "video_paths = [os.path.join(data_path, video) for video in os.listdir(data_path)]\n",
    "\n",
    "for video_path in video_paths:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        status, frame = cap.read()\n",
    "        \n",
    "        if not status:\n",
    "            break\n",
    "        \n",
    "        outputs = model(frame)\n",
    "        outputs = np.squeeze(outputs.render())\n",
    "        cv2.imshow('frame', outputs)\n",
    "\n",
    "        if cv2.waitKey(10) == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
