{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=0.0001, clipnorm=1.) # Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가중치 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier 초기화\n",
    "\n",
    "정규 분포와 균일 분포로 2가지의 경우로 나누어 사용되며, sigmoid 함수나 tanh와 같은 S자 형태의 활성화 함수와 함께 사용할 때 성능이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He 초기화\n",
    "\n",
    "마찬가지로 정규 분포와 균일 분포 2가지의 경우로 나누어 사용되며, 활성화 함수로 ReLU 계열을 사용할 때 성능이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 ReLU + HE 초기화가 더 보편적인 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배치 정규화\n",
    "\n",
    "가중치 초기화를 통해 초기에 Gradient Vanishing과 Exploding을 완화할 수 있지만, 훈련 중에 이 현상들이 다시 발생할 수 있다. 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다. 배치 정규화를 사용하면 가중치 초기화에 훨씬 덜 민감해지며, 훨씬 큰 학습률을 사용할 수 있고, 미니 배치마다 평균과 표준편차를 계산하여 Drop out과 비슷한 효과를 낸다. 다만, 미니 배치 크기가 너무 작을 경우와 RNN에는 적용하기 어렵다는 단점이 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
