{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA는 카운트 기반으로 corpus의 전체적인 통계 정보를 고려하지만, 단어 의미의 유추에는 성능이 떨어진다. Word2Vec은 예측 기반으로 단어 간 유추에는 성능이 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 corpus의 전체적인 통계 정보를 반영하지는 못한다. 이러한 문제를 해결하고자 두 가지 방법을 모두 사용한 GloVe가 제시되었다. GloVe(Global Vectors for Word Representation)는 카운트 기반과 예측 기반을 모두 사용하는 단어 임베딩 방법론이다. 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 corpus에서의 동시 등장 확률이 되도록 임베딩 벡터를 만느는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 윈도우 기반 동시 등장 행렬(Co-occurrence Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-occurrence Matrix는 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 특정한 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 작성한 행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동시 등장 확률(Co-Occurrence Probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ P(k|i) = \\frac{i행 k열}{i행 전체 합}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수(Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Loss Function = \\sum_{m, n = 1}^{V} f(X_mn)(w^T_m\\overset{\\sim}{w}_n + b_m + \\overset{\\sim}{b}_n - logX_mn)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# GloVe 패키지 다운로드가 되지 않는다..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
