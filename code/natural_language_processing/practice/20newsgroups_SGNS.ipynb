{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram with Negative Sampling을 사용한 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중심 단어, 주변 단어들을 포함해 최소 2개의 단어가 있어야 함.\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "news = dataset.data\n",
    "\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    well sure about story seem biased what disagre...\n",
       "1    yeah expect people read actually accept hard a...\n",
       "2    although realize that principle your strongest...\n",
       "3    notwithstanding legitimate fuss about this pro...\n",
       "4    well will have change scoring playoff pool unf...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특수 문자와 길이가 3이하인 단어 제거 및 소문자 변환\n",
    "news_df = pd.DataFrame({\"doc\": news})\n",
    "news_df[\"clean_doc\"] = news_df[\"doc\"].str.replace(\"[^a-zA-Z]\", \" \", regex=True) # 알파벳이 아닌 것 제거\n",
    "news_df[\"clean_doc\"] = news_df[\"clean_doc\"].apply(lambda doc: \" \".join([word for word in doc.split() if len(word) > 3])) # 길이가 3이하인 단어는 제거\n",
    "news_df[\"clean_doc\"] = news_df[\"clean_doc\"].str.lower() # 소문자 변환\n",
    "\n",
    "news_df[\"clean_doc\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc          False\n",
       "clean_doc    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "news_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc          True\n",
       "clean_doc    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈 값 확인\n",
    "news_df = news_df.replace(\"\", np.nan)\n",
    "news_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10995"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 삭제\n",
    "news_df.dropna(inplace=True)\n",
    "\n",
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "stop_words = stopwords.words(\"english\")\n",
    "tokenized_doc = news_df[\"clean_doc\"].apply(lambda doc: [word for word in doc.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어가 1개 이하인 경우 제거\n",
    "tokenized_doc = tokenized_doc.apply(lambda doc: doc if len(doc) > 1 else \"target\")\n",
    "mask = tokenized_doc == \"target\"\n",
    "idx = tokenized_doc[mask].index\n",
    "tokenized_doc = tokenized_doc.drop(idx)\n",
    "\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "print(encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGNS을 통한 dataset 구성\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acts (1102), beverly (11853) -> 0\n",
      "makes (228), underwhelming (45547) -> 0\n",
      "reason (149), commited (7837) -> 1\n",
      "shame (4988), lqavd (56298) -> 0\n",
      "blessing (10669), austria (4866) -> 1\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(f\"{idx2word[pairs[i][0]]} ({pairs[i][0]}), {idx2word[pairs[i][1]]} ({pairs[i][1]}) -> {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2220, 2220)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 100)       6427700     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 100)       6427700     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 1, 1)         0           ['embedding_2[0][0]',            \n",
      "                                                                  'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1)            0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1)            0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# SGNS\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# 중심 단어 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
    "\n",
    "# 주변 단어 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype=np.int32)\n",
    "context_embedding = Embedding(vocab_size, embedding_dim)(c_inputs)\n",
    "\n",
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1, ), input_shape=(1, 1))(dot_product)\n",
    "output = Activation(\"sigmoid\")(dot_product)\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "plot_model(model, to_file=\"20newsgroups.png\", show_shapes=True, show_layer_names=True, rankdir=\"TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 19:13:51.345281: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-28 19:13:51.616634: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.930021107196808\n",
      "Epoch: 1, Loss: 6.904720962047577\n",
      "Epoch: 2, Loss: 6.876796245574951\n",
      "Epoch: 3, Loss: 6.842806041240692\n",
      "Epoch: 4, Loss: 6.800464391708374\n",
      "Epoch: 5, Loss: 6.747705817222595\n",
      "Epoch: 6, Loss: 6.6827380657196045\n",
      "Epoch: 7, Loss: 6.604081630706787\n",
      "Epoch: 8, Loss: 6.5106083154678345\n",
      "Epoch: 9, Loss: 6.401570498943329\n",
      "Epoch: 10, Loss: 6.276617050170898\n",
      "Epoch: 11, Loss: 6.135801255702972\n",
      "Epoch: 12, Loss: 5.9795748591423035\n",
      "Epoch: 13, Loss: 5.808772087097168\n",
      "Epoch: 14, Loss: 5.624581754207611\n",
      "Epoch: 15, Loss: 5.428508639335632\n",
      "Epoch: 16, Loss: 5.222320914268494\n",
      "Epoch: 17, Loss: 5.007989436388016\n",
      "Epoch: 18, Loss: 4.787619888782501\n",
      "Epoch: 19, Loss: 4.5633814334869385\n",
      "Epoch: 20, Loss: 4.337436705827713\n",
      "Epoch: 21, Loss: 4.111874878406525\n",
      "Epoch: 22, Loss: 3.888652265071869\n",
      "Epoch: 23, Loss: 3.6695437133312225\n",
      "Epoch: 24, Loss: 3.456105053424835\n",
      "Epoch: 25, Loss: 3.2496496737003326\n",
      "Epoch: 26, Loss: 3.051237612962723\n",
      "Epoch: 27, Loss: 2.861675977706909\n",
      "Epoch: 28, Loss: 2.6815301179885864\n",
      "Epoch: 29, Loss: 2.5111425518989563\n",
      "Epoch: 30, Loss: 2.3506578356027603\n",
      "Epoch: 31, Loss: 2.2000504583120346\n",
      "Epoch: 32, Loss: 2.0591535717248917\n",
      "Epoch: 33, Loss: 1.9276883155107498\n",
      "Epoch: 34, Loss: 1.8052905797958374\n",
      "Epoch: 35, Loss: 1.6915348172187805\n",
      "Epoch: 36, Loss: 1.585955649614334\n",
      "Epoch: 37, Loss: 1.4880651235580444\n",
      "Epoch: 38, Loss: 1.3973670974373817\n",
      "Epoch: 39, Loss: 1.313368797302246\n",
      "Epoch: 40, Loss: 1.2355890348553658\n",
      "Epoch: 41, Loss: 1.163564808666706\n",
      "Epoch: 42, Loss: 1.0968552604317665\n",
      "Epoch: 43, Loss: 1.0350445359945297\n",
      "Epoch: 44, Loss: 0.9777432456612587\n",
      "Epoch: 45, Loss: 0.9245887100696564\n",
      "Epoch: 46, Loss: 0.8752449750900269\n",
      "Epoch: 47, Loss: 0.8294020146131516\n",
      "Epoch: 48, Loss: 0.7867743521928787\n",
      "Epoch: 49, Loss: 0.7471001222729683\n",
      "Epoch: 50, Loss: 0.7101393640041351\n",
      "Epoch: 51, Loss: 0.6756725013256073\n",
      "Epoch: 52, Loss: 0.6434987969696522\n",
      "Epoch: 53, Loss: 0.6134351119399071\n",
      "Epoch: 54, Loss: 0.5853140950202942\n",
      "Epoch: 55, Loss: 0.5589830875396729\n",
      "Epoch: 56, Loss: 0.5343027338385582\n",
      "Epoch: 57, Loss: 0.5111458860337734\n",
      "Epoch: 58, Loss: 0.489396370947361\n",
      "Epoch: 59, Loss: 0.4689481593668461\n",
      "Epoch: 60, Loss: 0.44970426335930824\n",
      "Epoch: 61, Loss: 0.43157607316970825\n",
      "Epoch: 62, Loss: 0.414482444524765\n",
      "Epoch: 63, Loss: 0.3983491137623787\n",
      "Epoch: 64, Loss: 0.3831080198287964\n",
      "Epoch: 65, Loss: 0.3686967007815838\n",
      "Epoch: 66, Loss: 0.3550578970462084\n",
      "Epoch: 67, Loss: 0.3421389274299145\n",
      "Epoch: 68, Loss: 0.32989138923585415\n",
      "Epoch: 69, Loss: 0.3182707391679287\n",
      "Epoch: 70, Loss: 0.30723592825233936\n",
      "Epoch: 71, Loss: 0.29674912989139557\n",
      "Epoch: 72, Loss: 0.2867753952741623\n",
      "Epoch: 73, Loss: 0.2772824913263321\n",
      "Epoch: 74, Loss: 0.26824059523642063\n",
      "Epoch: 75, Loss: 0.2596220877021551\n",
      "Epoch: 76, Loss: 0.2514014355838299\n",
      "Epoch: 77, Loss: 0.24355489574372768\n",
      "Epoch: 78, Loss: 0.23606052622199059\n",
      "Epoch: 79, Loss: 0.22889788635075092\n",
      "Epoch: 80, Loss: 0.2220479678362608\n",
      "Epoch: 81, Loss: 0.21549313515424728\n",
      "Epoch: 82, Loss: 0.20921691693365574\n",
      "Epoch: 83, Loss: 0.2032039873301983\n",
      "Epoch: 84, Loss: 0.1974400393664837\n",
      "Epoch: 85, Loss: 0.1919117234647274\n",
      "Epoch: 86, Loss: 0.18660656176507473\n",
      "Epoch: 87, Loss: 0.18151288200169802\n",
      "Epoch: 88, Loss: 0.176619753241539\n",
      "Epoch: 89, Loss: 0.17191695608198643\n",
      "Epoch: 90, Loss: 0.16739489044994116\n",
      "Epoch: 91, Loss: 0.16304456256330013\n",
      "Epoch: 92, Loss: 0.15885752066969872\n",
      "Epoch: 93, Loss: 0.15482581965625286\n",
      "Epoch: 94, Loss: 0.15094201173633337\n",
      "Epoch: 95, Loss: 0.14719903655350208\n",
      "Epoch: 96, Loss: 0.14359029941260815\n",
      "Epoch: 97, Loss: 0.1401095651090145\n",
      "Epoch: 98, Loss: 0.13675096165388823\n",
      "Epoch: 99, Loss: 0.13350891415029764\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype=np.int32)\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype=np.int32)\n",
    "        labels = np.array(elem[1], dtype=np.int32)\n",
    "        X = [first_elem, second_elem]\n",
    "        y = labels\n",
    "        loss += model.train_on_batch(X, y)\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "import gensim\n",
    "\n",
    "with open(\"vectors.txt\", \"w\") as f:\n",
    "    f.write(f\"{vocab_size - 1} {embedding_dim}\\n\")\n",
    "    vectors = model.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(f\"{word}, {' '.join(map(str, list(vectors[i, :])))}\\n\")\n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(\"./vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'people' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w2v\u001b[39m.\u001b[39;49mmost_similar(positive\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mpeople\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[39m=\u001b[39m item[\u001b[39m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[39m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_mean_vector(keys, weight, pre_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, post_normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, ignore_missing\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    842\u001b[0m all_keys \u001b[39m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_index(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, _KEY_TYPES) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[39mif\u001b[39;00m indexer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(topn, \u001b[39mint\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/gensim/models/keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m         total_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(weights[idx])\n\u001b[1;32m    517\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_missing:\n\u001b[0;32m--> 518\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present in vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m total_weight \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    521\u001b[0m     mean \u001b[39m=\u001b[39m mean \u001b[39m/\u001b[39m total_weight\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'people' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "w2v.most_similar(positive=[\"people\"]) # 이 부분은 오류 해결이 어려워서 실패..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
