{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문자 단위 RNN 언어 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
    "\n",
    "sentences = []\n",
    "with open(\"11-0.txt\") as f:\n",
    "    for sentence in f:\n",
    "        # 공백, 바이트 열 제거 및 소문자화\n",
    "        sentence = sentence.strip().lower().encode().decode(\"ascii\", \"ignore\")\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and',\n",
       " 'most other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever. you may copy it, give it away or re-use it under the terms',\n",
       " 'of the project gutenberg license included with this ebook or online at']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159484, 56)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = \" \".join(sentences)\n",
    "char_vocab = sorted(list(set(total_data)))\n",
    "vocab_size = len(char_vocab)\n",
    "\n",
    "len(total_data), vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2658"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터 구성\n",
    "seq_len = 60 # 한 문장의 길이를 60으로 설정\n",
    "n_samples = int(np.floor((len(total_data) - 1) / seq_len))\n",
    "\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    X_sample = total_data[i * seq_len:(i + 1) * seq_len]\n",
    "    X_encoded = [char_to_idx[char] for char in X_sample] # 정수 인코딩\n",
    "    X_train.append(X_encoded)\n",
    "\n",
    "    y_sample = total_data[i * seq_len + 1:(i + 1) * seq_len + 1] # 한 문자 shift\n",
    "    y_encoded = [char_to_idx[char] for char in y_sample]\n",
    "    y_train.append(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2658, 60, 56), (2658, 60, 56))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문자 단위 RNN에서는 Word Embedding을 사용하지 않고 OHE를 사용\n",
    "X_train_ohe = to_categorical(X_train)\n",
    "y_train_ohe = to_categorical(y_train)\n",
    "\n",
    "X_train_ohe.shape, y_train_ohe.shape # batch size: 2658, timestpes: 60, input_dim: 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 20:29:50.389069: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-18 20:29:50.688806: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-18 20:29:50.875779: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-18 20:29:51.130384: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-18 20:29:51.401864: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 - 7s - loss: 3.0593 - accuracy: 0.1826 - 7s/epoch - 81ms/step\n",
      "Epoch 2/80\n",
      "84/84 - 4s - loss: 2.6972 - accuracy: 0.2579 - 4s/epoch - 42ms/step\n",
      "Epoch 3/80\n",
      "84/84 - 4s - loss: 2.3825 - accuracy: 0.3290 - 4s/epoch - 44ms/step\n",
      "Epoch 4/80\n",
      "84/84 - 4s - loss: 2.2483 - accuracy: 0.3605 - 4s/epoch - 43ms/step\n",
      "Epoch 5/80\n",
      "84/84 - 4s - loss: 2.1442 - accuracy: 0.3847 - 4s/epoch - 44ms/step\n",
      "Epoch 6/80\n",
      "84/84 - 4s - loss: 2.0639 - accuracy: 0.4057 - 4s/epoch - 45ms/step\n",
      "Epoch 7/80\n",
      "84/84 - 4s - loss: 2.0037 - accuracy: 0.4221 - 4s/epoch - 46ms/step\n",
      "Epoch 8/80\n",
      "84/84 - 4s - loss: 1.9405 - accuracy: 0.4392 - 4s/epoch - 46ms/step\n",
      "Epoch 9/80\n",
      "84/84 - 4s - loss: 1.8908 - accuracy: 0.4517 - 4s/epoch - 47ms/step\n",
      "Epoch 10/80\n",
      "84/84 - 4s - loss: 1.8488 - accuracy: 0.4628 - 4s/epoch - 48ms/step\n",
      "Epoch 11/80\n",
      "84/84 - 4s - loss: 1.8040 - accuracy: 0.4727 - 4s/epoch - 48ms/step\n",
      "Epoch 12/80\n",
      "84/84 - 4s - loss: 1.7634 - accuracy: 0.4847 - 4s/epoch - 48ms/step\n",
      "Epoch 13/80\n",
      "84/84 - 4s - loss: 1.7276 - accuracy: 0.4957 - 4s/epoch - 49ms/step\n",
      "Epoch 14/80\n",
      "84/84 - 4s - loss: 1.6922 - accuracy: 0.5069 - 4s/epoch - 49ms/step\n",
      "Epoch 15/80\n",
      "84/84 - 4s - loss: 1.6557 - accuracy: 0.5160 - 4s/epoch - 50ms/step\n",
      "Epoch 16/80\n",
      "84/84 - 4s - loss: 1.6265 - accuracy: 0.5231 - 4s/epoch - 50ms/step\n",
      "Epoch 17/80\n",
      "84/84 - 4s - loss: 1.5953 - accuracy: 0.5310 - 4s/epoch - 51ms/step\n",
      "Epoch 18/80\n",
      "84/84 - 4s - loss: 1.5640 - accuracy: 0.5396 - 4s/epoch - 51ms/step\n",
      "Epoch 19/80\n",
      "84/84 - 4s - loss: 1.5386 - accuracy: 0.5453 - 4s/epoch - 52ms/step\n",
      "Epoch 20/80\n",
      "84/84 - 4s - loss: 1.5108 - accuracy: 0.5537 - 4s/epoch - 52ms/step\n",
      "Epoch 21/80\n",
      "84/84 - 4s - loss: 1.4812 - accuracy: 0.5621 - 4s/epoch - 52ms/step\n",
      "Epoch 22/80\n",
      "84/84 - 4s - loss: 1.4541 - accuracy: 0.5684 - 4s/epoch - 52ms/step\n",
      "Epoch 23/80\n",
      "84/84 - 5s - loss: 1.4257 - accuracy: 0.5772 - 5s/epoch - 55ms/step\n",
      "Epoch 24/80\n",
      "84/84 - 4s - loss: 1.3987 - accuracy: 0.5852 - 4s/epoch - 53ms/step\n",
      "Epoch 25/80\n",
      "84/84 - 5s - loss: 1.3744 - accuracy: 0.5917 - 5s/epoch - 54ms/step\n",
      "Epoch 26/80\n",
      "84/84 - 5s - loss: 1.3469 - accuracy: 0.5991 - 5s/epoch - 55ms/step\n",
      "Epoch 27/80\n",
      "84/84 - 5s - loss: 1.3240 - accuracy: 0.6047 - 5s/epoch - 54ms/step\n",
      "Epoch 28/80\n",
      "84/84 - 5s - loss: 1.2967 - accuracy: 0.6124 - 5s/epoch - 55ms/step\n",
      "Epoch 29/80\n",
      "84/84 - 5s - loss: 1.2725 - accuracy: 0.6194 - 5s/epoch - 54ms/step\n",
      "Epoch 30/80\n",
      "84/84 - 5s - loss: 1.2458 - accuracy: 0.6278 - 5s/epoch - 55ms/step\n",
      "Epoch 31/80\n",
      "84/84 - 5s - loss: 1.2190 - accuracy: 0.6362 - 5s/epoch - 55ms/step\n",
      "Epoch 32/80\n",
      "84/84 - 5s - loss: 1.1937 - accuracy: 0.6428 - 5s/epoch - 56ms/step\n",
      "Epoch 33/80\n",
      "84/84 - 5s - loss: 1.1691 - accuracy: 0.6498 - 5s/epoch - 57ms/step\n",
      "Epoch 34/80\n",
      "84/84 - 5s - loss: 1.1404 - accuracy: 0.6579 - 5s/epoch - 58ms/step\n",
      "Epoch 35/80\n",
      "84/84 - 5s - loss: 1.1169 - accuracy: 0.6653 - 5s/epoch - 59ms/step\n",
      "Epoch 36/80\n",
      "84/84 - 5s - loss: 1.0889 - accuracy: 0.6741 - 5s/epoch - 61ms/step\n",
      "Epoch 37/80\n",
      "84/84 - 5s - loss: 1.0607 - accuracy: 0.6824 - 5s/epoch - 62ms/step\n",
      "Epoch 38/80\n",
      "84/84 - 5s - loss: 1.0375 - accuracy: 0.6894 - 5s/epoch - 64ms/step\n",
      "Epoch 39/80\n",
      "84/84 - 5s - loss: 1.0107 - accuracy: 0.6978 - 5s/epoch - 65ms/step\n",
      "Epoch 40/80\n",
      "84/84 - 6s - loss: 0.9889 - accuracy: 0.7033 - 6s/epoch - 67ms/step\n",
      "Epoch 41/80\n",
      "84/84 - 6s - loss: 0.9561 - accuracy: 0.7148 - 6s/epoch - 68ms/step\n",
      "Epoch 42/80\n",
      "84/84 - 6s - loss: 0.9295 - accuracy: 0.7220 - 6s/epoch - 68ms/step\n",
      "Epoch 43/80\n",
      "84/84 - 6s - loss: 0.9090 - accuracy: 0.7288 - 6s/epoch - 70ms/step\n",
      "Epoch 44/80\n",
      "84/84 - 6s - loss: 0.8785 - accuracy: 0.7389 - 6s/epoch - 71ms/step\n",
      "Epoch 45/80\n",
      "84/84 - 7s - loss: 0.8563 - accuracy: 0.7451 - 7s/epoch - 85ms/step\n",
      "Epoch 46/80\n",
      "84/84 - 7s - loss: 0.8318 - accuracy: 0.7529 - 7s/epoch - 82ms/step\n",
      "Epoch 47/80\n",
      "84/84 - 7s - loss: 0.8024 - accuracy: 0.7626 - 7s/epoch - 89ms/step\n",
      "Epoch 48/80\n",
      "84/84 - 7s - loss: 0.7794 - accuracy: 0.7699 - 7s/epoch - 83ms/step\n",
      "Epoch 49/80\n",
      "84/84 - 7s - loss: 0.7611 - accuracy: 0.7754 - 7s/epoch - 83ms/step\n",
      "Epoch 50/80\n",
      "84/84 - 7s - loss: 0.7278 - accuracy: 0.7859 - 7s/epoch - 87ms/step\n",
      "Epoch 51/80\n",
      "84/84 - 7s - loss: 0.7126 - accuracy: 0.7907 - 7s/epoch - 86ms/step\n",
      "Epoch 52/80\n",
      "84/84 - 7s - loss: 0.6852 - accuracy: 0.7997 - 7s/epoch - 84ms/step\n",
      "Epoch 53/80\n",
      "84/84 - 7s - loss: 0.6563 - accuracy: 0.8100 - 7s/epoch - 85ms/step\n",
      "Epoch 54/80\n",
      "84/84 - 7s - loss: 0.6409 - accuracy: 0.8143 - 7s/epoch - 83ms/step\n",
      "Epoch 55/80\n",
      "84/84 - 7s - loss: 0.6126 - accuracy: 0.8238 - 7s/epoch - 86ms/step\n",
      "Epoch 56/80\n",
      "84/84 - 7s - loss: 0.5952 - accuracy: 0.8292 - 7s/epoch - 80ms/step\n",
      "Epoch 57/80\n",
      "84/84 - 7s - loss: 0.5741 - accuracy: 0.8355 - 7s/epoch - 87ms/step\n",
      "Epoch 58/80\n",
      "84/84 - 7s - loss: 0.5573 - accuracy: 0.8407 - 7s/epoch - 82ms/step\n",
      "Epoch 59/80\n",
      "84/84 - 7s - loss: 0.5348 - accuracy: 0.8480 - 7s/epoch - 81ms/step\n",
      "Epoch 60/80\n",
      "84/84 - 7s - loss: 0.5142 - accuracy: 0.8547 - 7s/epoch - 85ms/step\n",
      "Epoch 61/80\n",
      "84/84 - 7s - loss: 0.5042 - accuracy: 0.8577 - 7s/epoch - 81ms/step\n",
      "Epoch 62/80\n",
      "84/84 - 7s - loss: 0.4883 - accuracy: 0.8623 - 7s/epoch - 84ms/step\n",
      "Epoch 63/80\n",
      "84/84 - 8s - loss: 0.4625 - accuracy: 0.8709 - 8s/epoch - 91ms/step\n",
      "Epoch 64/80\n",
      "84/84 - 7s - loss: 0.4557 - accuracy: 0.8726 - 7s/epoch - 81ms/step\n",
      "Epoch 65/80\n",
      "84/84 - 7s - loss: 0.4300 - accuracy: 0.8819 - 7s/epoch - 80ms/step\n",
      "Epoch 66/80\n",
      "84/84 - 7s - loss: 0.4136 - accuracy: 0.8875 - 7s/epoch - 85ms/step\n",
      "Epoch 67/80\n",
      "84/84 - 7s - loss: 0.3875 - accuracy: 0.8966 - 7s/epoch - 84ms/step\n",
      "Epoch 68/80\n",
      "84/84 - 7s - loss: 0.3802 - accuracy: 0.8981 - 7s/epoch - 82ms/step\n",
      "Epoch 69/80\n",
      "84/84 - 7s - loss: 0.3645 - accuracy: 0.9032 - 7s/epoch - 82ms/step\n",
      "Epoch 70/80\n",
      "84/84 - 7s - loss: 0.3544 - accuracy: 0.9057 - 7s/epoch - 85ms/step\n",
      "Epoch 71/80\n",
      "84/84 - 7s - loss: 0.3349 - accuracy: 0.9130 - 7s/epoch - 86ms/step\n",
      "Epoch 72/80\n",
      "84/84 - 7s - loss: 0.3177 - accuracy: 0.9189 - 7s/epoch - 88ms/step\n",
      "Epoch 73/80\n",
      "84/84 - 7s - loss: 0.3636 - accuracy: 0.8970 - 7s/epoch - 85ms/step\n",
      "Epoch 74/80\n",
      "84/84 - 7s - loss: 0.3130 - accuracy: 0.9183 - 7s/epoch - 81ms/step\n",
      "Epoch 75/80\n",
      "84/84 - 7s - loss: 0.2975 - accuracy: 0.9241 - 7s/epoch - 84ms/step\n",
      "Epoch 76/80\n",
      "84/84 - 7s - loss: 0.2779 - accuracy: 0.9305 - 7s/epoch - 86ms/step\n",
      "Epoch 77/80\n",
      "84/84 - 7s - loss: 0.2775 - accuracy: 0.9304 - 7s/epoch - 82ms/step\n",
      "Epoch 78/80\n",
      "84/84 - 7s - loss: 0.2595 - accuracy: 0.9361 - 7s/epoch - 81ms/step\n",
      "Epoch 79/80\n",
      "84/84 - 7s - loss: 0.2522 - accuracy: 0.9389 - 7s/epoch - 81ms/step\n",
      "Epoch 80/80\n",
      "84/84 - 7s - loss: 0.2443 - accuracy: 0.9402 - 7s/epoch - 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c363d490>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modeling\n",
    "hiddent_size = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hiddent_size, input_shape=(None, X_train_ohe.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(hiddent_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation=\"softmax\")))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train_ohe, y_train_ohe, epochs=80, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51번 문자 v로 예측 시작\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ve raglgviyaaaeiiyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sentence(model, length):\n",
    "    \"\"\"특정 문자를 전달 받아 다음 문자를 계속 생성하는 함수\"\"\"\n",
    "    idx = [np.random.randint(vocab_size)]\n",
    "    y_char = [idx_to_char[idx[-1]]]\n",
    "    print(f\"{idx[-1]}번 문자 {y_char[0]}로 예측 시작\")\n",
    "    X = np.zeros((1, length, vocab_size)) # LSTM의 입력 시퀀스\n",
    "\n",
    "    for i in range(length):\n",
    "        X[0][i][idx] = 1 # 예측 문자의 인덱스를 입력 시퀀스에 추가\n",
    "        print(idx_to_char[idx[-1]], end=\"\")\n",
    "        idx = np.argmax(model.predict(X[:, :i + 1, :])[0], axis=1)\n",
    "        y_char.append(idx_to_char[idx[-1]])\n",
    "    return \"\".join(y_char)\n",
    "\n",
    "res = generate_sentence(model, 100)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다대일 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = '''\n",
    "I get on with life as a programmer,\n",
    "I like to contemplate beer.\n",
    "But when I start to daydream,\n",
    "My mind turns straight to wine.\n",
    "\n",
    "Do I love wine more than beer?\n",
    "\n",
    "I like to use words about beer.\n",
    "But when I stop my talking,\n",
    "My mind turns straight to wine.\n",
    "\n",
    "I hate bugs and errors.\n",
    "But I just think back to wine,\n",
    "And I'm happy once again.\n",
    "\n",
    "I like to hang out with programming and deep learning.\n",
    "But when left alone,\n",
    "My mind turns straight to wine.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하나의 문자열로\n",
    "tokens = raw_text.split()\n",
    "raw_text = \" \".join(tokens)\n",
    "\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', \"'\", ',', '.', '?', 'A', 'B', 'D', 'I', 'M', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# 중복을 제거한 문자 집합 생성\n",
    "char_vocab = sorted(list(set(raw_text)))\n",
    "vocab_size = len(char_vocab)\n",
    "\n",
    "print(char_vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, \"'\": 1, ',': 2, '.': 3, '?': 4, 'A': 5, 'B': 6, 'D': 7, 'I': 8, 'M': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32}\n"
     ]
    }
   ],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
    "\n",
    "print(char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I get on wi',\n",
       " ' get on wit',\n",
       " 'get on with',\n",
       " 'et on with ',\n",
       " 't on with l',\n",
       " ' on with li',\n",
       " 'on with lif',\n",
       " 'n with life',\n",
       " ' with life ',\n",
       " 'with life a']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# timesteps = 10\n",
    "length = 11\n",
    "sequences = []\n",
    "\n",
    "for i in range(length, len(raw_text)):\n",
    "    seq = raw_text[i - length: i] \n",
    "    sequences.append(seq)\n",
    "\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18],\n",
       " [0, 16, 14, 28, 0, 24, 23, 0, 31, 18, 28],\n",
       " [16, 14, 28, 0, 24, 23, 0, 31, 18, 28, 17],\n",
       " [14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0],\n",
       " [28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences = []\n",
    "\n",
    "for seq in sequences:\n",
    "    encoded_sequence = [char_to_idx[char] for char in seq]\n",
    "    encoded_sequences.append(encoded_sequence)\n",
    "\n",
    "encoded_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  0, 16, 14, 28,  0, 24, 23,  0, 31],\n",
       "       [ 0, 16, 14, 28,  0, 24, 23,  0, 31, 18],\n",
       "       [16, 14, 28,  0, 24, 23,  0, 31, 18, 28],\n",
       "       [14, 28,  0, 24, 23,  0, 31, 18, 28, 17],\n",
       "       [28,  0, 24, 23,  0, 31, 18, 28, 17,  0]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sequences = np.array(encoded_sequences)\n",
    "X_train = encoded_sequences[:, :-1]\n",
    "y_train = encoded_sequences[:, -1]\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE\n",
    "X_train_ohe = np.array([to_categorical(X, num_classes=vocab_size) for X in X_train])\n",
    "y_train_ohe = to_categorical(y_train, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 10, 33)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 19:49:33.340695: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-19 19:49:33.487582: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-19 19:49:33.598965: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 1s - loss: 3.4621 - accuracy: 0.1338 - 1s/epoch - 97ms/step\n",
      "Epoch 2/100\n",
      "14/14 - 0s - loss: 3.3339 - accuracy: 0.1972 - 264ms/epoch - 19ms/step\n",
      "Epoch 3/100\n",
      "14/14 - 0s - loss: 3.0535 - accuracy: 0.1972 - 242ms/epoch - 17ms/step\n",
      "Epoch 4/100\n",
      "14/14 - 0s - loss: 2.9835 - accuracy: 0.1972 - 248ms/epoch - 18ms/step\n",
      "Epoch 5/100\n",
      "14/14 - 0s - loss: 2.9585 - accuracy: 0.1972 - 234ms/epoch - 17ms/step\n",
      "Epoch 6/100\n",
      "14/14 - 0s - loss: 2.9451 - accuracy: 0.1972 - 226ms/epoch - 16ms/step\n",
      "Epoch 7/100\n",
      "14/14 - 0s - loss: 2.9257 - accuracy: 0.1972 - 285ms/epoch - 20ms/step\n",
      "Epoch 8/100\n",
      "14/14 - 0s - loss: 2.9123 - accuracy: 0.1972 - 273ms/epoch - 20ms/step\n",
      "Epoch 9/100\n",
      "14/14 - 0s - loss: 2.9043 - accuracy: 0.1972 - 252ms/epoch - 18ms/step\n",
      "Epoch 10/100\n",
      "14/14 - 0s - loss: 2.8707 - accuracy: 0.1972 - 257ms/epoch - 18ms/step\n",
      "Epoch 11/100\n",
      "14/14 - 0s - loss: 2.8429 - accuracy: 0.1995 - 224ms/epoch - 16ms/step\n",
      "Epoch 12/100\n",
      "14/14 - 0s - loss: 2.8223 - accuracy: 0.2019 - 256ms/epoch - 18ms/step\n",
      "Epoch 13/100\n",
      "14/14 - 0s - loss: 2.7845 - accuracy: 0.2066 - 246ms/epoch - 18ms/step\n",
      "Epoch 14/100\n",
      "14/14 - 0s - loss: 2.7566 - accuracy: 0.2019 - 243ms/epoch - 17ms/step\n",
      "Epoch 15/100\n",
      "14/14 - 0s - loss: 2.7276 - accuracy: 0.2113 - 232ms/epoch - 17ms/step\n",
      "Epoch 16/100\n",
      "14/14 - 0s - loss: 2.7019 - accuracy: 0.2324 - 289ms/epoch - 21ms/step\n",
      "Epoch 17/100\n",
      "14/14 - 0s - loss: 2.6676 - accuracy: 0.2324 - 284ms/epoch - 20ms/step\n",
      "Epoch 18/100\n",
      "14/14 - 0s - loss: 2.6263 - accuracy: 0.2488 - 259ms/epoch - 19ms/step\n",
      "Epoch 19/100\n",
      "14/14 - 0s - loss: 2.5788 - accuracy: 0.2535 - 226ms/epoch - 16ms/step\n",
      "Epoch 20/100\n",
      "14/14 - 0s - loss: 2.5377 - accuracy: 0.2911 - 328ms/epoch - 23ms/step\n",
      "Epoch 21/100\n",
      "14/14 - 0s - loss: 2.5021 - accuracy: 0.2958 - 224ms/epoch - 16ms/step\n",
      "Epoch 22/100\n",
      "14/14 - 0s - loss: 2.4578 - accuracy: 0.3099 - 265ms/epoch - 19ms/step\n",
      "Epoch 23/100\n",
      "14/14 - 0s - loss: 2.4411 - accuracy: 0.3099 - 226ms/epoch - 16ms/step\n",
      "Epoch 24/100\n",
      "14/14 - 0s - loss: 2.3971 - accuracy: 0.3545 - 272ms/epoch - 19ms/step\n",
      "Epoch 25/100\n",
      "14/14 - 0s - loss: 2.3592 - accuracy: 0.3169 - 254ms/epoch - 18ms/step\n",
      "Epoch 26/100\n",
      "14/14 - 0s - loss: 2.3399 - accuracy: 0.3216 - 246ms/epoch - 18ms/step\n",
      "Epoch 27/100\n",
      "14/14 - 0s - loss: 2.2830 - accuracy: 0.3498 - 313ms/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "14/14 - 0s - loss: 2.2447 - accuracy: 0.3498 - 268ms/epoch - 19ms/step\n",
      "Epoch 29/100\n",
      "14/14 - 0s - loss: 2.2100 - accuracy: 0.3803 - 235ms/epoch - 17ms/step\n",
      "Epoch 30/100\n",
      "14/14 - 0s - loss: 2.1590 - accuracy: 0.3944 - 263ms/epoch - 19ms/step\n",
      "Epoch 31/100\n",
      "14/14 - 0s - loss: 2.1177 - accuracy: 0.3732 - 233ms/epoch - 17ms/step\n",
      "Epoch 32/100\n",
      "14/14 - 0s - loss: 2.0796 - accuracy: 0.4038 - 261ms/epoch - 19ms/step\n",
      "Epoch 33/100\n",
      "14/14 - 0s - loss: 2.0310 - accuracy: 0.3920 - 222ms/epoch - 16ms/step\n",
      "Epoch 34/100\n",
      "14/14 - 0s - loss: 2.0239 - accuracy: 0.4319 - 216ms/epoch - 15ms/step\n",
      "Epoch 35/100\n",
      "14/14 - 0s - loss: 1.9811 - accuracy: 0.4108 - 217ms/epoch - 16ms/step\n",
      "Epoch 36/100\n",
      "14/14 - 0s - loss: 1.9134 - accuracy: 0.4437 - 260ms/epoch - 19ms/step\n",
      "Epoch 37/100\n",
      "14/14 - 0s - loss: 1.8728 - accuracy: 0.4883 - 240ms/epoch - 17ms/step\n",
      "Epoch 38/100\n",
      "14/14 - 0s - loss: 1.8391 - accuracy: 0.4765 - 266ms/epoch - 19ms/step\n",
      "Epoch 39/100\n",
      "14/14 - 0s - loss: 1.7958 - accuracy: 0.4906 - 234ms/epoch - 17ms/step\n",
      "Epoch 40/100\n",
      "14/14 - 0s - loss: 1.7568 - accuracy: 0.5329 - 232ms/epoch - 17ms/step\n",
      "Epoch 41/100\n",
      "14/14 - 0s - loss: 1.7159 - accuracy: 0.5188 - 266ms/epoch - 19ms/step\n",
      "Epoch 42/100\n",
      "14/14 - 0s - loss: 1.6730 - accuracy: 0.5141 - 278ms/epoch - 20ms/step\n",
      "Epoch 43/100\n",
      "14/14 - 0s - loss: 1.6516 - accuracy: 0.5634 - 258ms/epoch - 18ms/step\n",
      "Epoch 44/100\n",
      "14/14 - 0s - loss: 1.6265 - accuracy: 0.5282 - 254ms/epoch - 18ms/step\n",
      "Epoch 45/100\n",
      "14/14 - 0s - loss: 1.5895 - accuracy: 0.5822 - 223ms/epoch - 16ms/step\n",
      "Epoch 46/100\n",
      "14/14 - 0s - loss: 1.5561 - accuracy: 0.5751 - 281ms/epoch - 20ms/step\n",
      "Epoch 47/100\n",
      "14/14 - 0s - loss: 1.5066 - accuracy: 0.6103 - 252ms/epoch - 18ms/step\n",
      "Epoch 48/100\n",
      "14/14 - 0s - loss: 1.4575 - accuracy: 0.6502 - 230ms/epoch - 16ms/step\n",
      "Epoch 49/100\n",
      "14/14 - 0s - loss: 1.4405 - accuracy: 0.6362 - 222ms/epoch - 16ms/step\n",
      "Epoch 50/100\n",
      "14/14 - 0s - loss: 1.4032 - accuracy: 0.6526 - 218ms/epoch - 16ms/step\n",
      "Epoch 51/100\n",
      "14/14 - 0s - loss: 1.3884 - accuracy: 0.6338 - 218ms/epoch - 16ms/step\n",
      "Epoch 52/100\n",
      "14/14 - 0s - loss: 1.3483 - accuracy: 0.6620 - 218ms/epoch - 16ms/step\n",
      "Epoch 53/100\n",
      "14/14 - 0s - loss: 1.2926 - accuracy: 0.6714 - 235ms/epoch - 17ms/step\n",
      "Epoch 54/100\n",
      "14/14 - 0s - loss: 1.2784 - accuracy: 0.6714 - 216ms/epoch - 15ms/step\n",
      "Epoch 55/100\n",
      "14/14 - 0s - loss: 1.2369 - accuracy: 0.6995 - 222ms/epoch - 16ms/step\n",
      "Epoch 56/100\n",
      "14/14 - 0s - loss: 1.2097 - accuracy: 0.7042 - 221ms/epoch - 16ms/step\n",
      "Epoch 57/100\n",
      "14/14 - 0s - loss: 1.1680 - accuracy: 0.7230 - 218ms/epoch - 16ms/step\n",
      "Epoch 58/100\n",
      "14/14 - 0s - loss: 1.1414 - accuracy: 0.7230 - 222ms/epoch - 16ms/step\n",
      "Epoch 59/100\n",
      "14/14 - 0s - loss: 1.1178 - accuracy: 0.7535 - 225ms/epoch - 16ms/step\n",
      "Epoch 60/100\n",
      "14/14 - 0s - loss: 1.0909 - accuracy: 0.7606 - 221ms/epoch - 16ms/step\n",
      "Epoch 61/100\n",
      "14/14 - 0s - loss: 1.0631 - accuracy: 0.7606 - 217ms/epoch - 15ms/step\n",
      "Epoch 62/100\n",
      "14/14 - 0s - loss: 1.0312 - accuracy: 0.7770 - 219ms/epoch - 16ms/step\n",
      "Epoch 63/100\n",
      "14/14 - 0s - loss: 0.9937 - accuracy: 0.7934 - 223ms/epoch - 16ms/step\n",
      "Epoch 64/100\n",
      "14/14 - 0s - loss: 0.9699 - accuracy: 0.7934 - 225ms/epoch - 16ms/step\n",
      "Epoch 65/100\n",
      "14/14 - 0s - loss: 0.9594 - accuracy: 0.7911 - 226ms/epoch - 16ms/step\n",
      "Epoch 66/100\n",
      "14/14 - 0s - loss: 0.9287 - accuracy: 0.7864 - 219ms/epoch - 16ms/step\n",
      "Epoch 67/100\n",
      "14/14 - 0s - loss: 0.8893 - accuracy: 0.8028 - 218ms/epoch - 16ms/step\n",
      "Epoch 68/100\n",
      "14/14 - 0s - loss: 0.8663 - accuracy: 0.8451 - 224ms/epoch - 16ms/step\n",
      "Epoch 69/100\n",
      "14/14 - 0s - loss: 0.8439 - accuracy: 0.8310 - 224ms/epoch - 16ms/step\n",
      "Epoch 70/100\n",
      "14/14 - 0s - loss: 0.8148 - accuracy: 0.8357 - 220ms/epoch - 16ms/step\n",
      "Epoch 71/100\n",
      "14/14 - 0s - loss: 0.8033 - accuracy: 0.8451 - 218ms/epoch - 16ms/step\n",
      "Epoch 72/100\n",
      "14/14 - 0s - loss: 0.7713 - accuracy: 0.8521 - 220ms/epoch - 16ms/step\n",
      "Epoch 73/100\n",
      "14/14 - 0s - loss: 0.7661 - accuracy: 0.8615 - 220ms/epoch - 16ms/step\n",
      "Epoch 74/100\n",
      "14/14 - 0s - loss: 0.7427 - accuracy: 0.8404 - 230ms/epoch - 16ms/step\n",
      "Epoch 75/100\n",
      "14/14 - 0s - loss: 0.7069 - accuracy: 0.8685 - 224ms/epoch - 16ms/step\n",
      "Epoch 76/100\n",
      "14/14 - 0s - loss: 0.6883 - accuracy: 0.8685 - 225ms/epoch - 16ms/step\n",
      "Epoch 77/100\n",
      "14/14 - 0s - loss: 0.6524 - accuracy: 0.8897 - 218ms/epoch - 16ms/step\n",
      "Epoch 78/100\n",
      "14/14 - 0s - loss: 0.6390 - accuracy: 0.8897 - 224ms/epoch - 16ms/step\n",
      "Epoch 79/100\n",
      "14/14 - 0s - loss: 0.6278 - accuracy: 0.8991 - 229ms/epoch - 16ms/step\n",
      "Epoch 80/100\n",
      "14/14 - 0s - loss: 0.6151 - accuracy: 0.9038 - 224ms/epoch - 16ms/step\n",
      "Epoch 81/100\n",
      "14/14 - 0s - loss: 0.5807 - accuracy: 0.9178 - 220ms/epoch - 16ms/step\n",
      "Epoch 82/100\n",
      "14/14 - 0s - loss: 0.5603 - accuracy: 0.9202 - 220ms/epoch - 16ms/step\n",
      "Epoch 83/100\n",
      "14/14 - 0s - loss: 0.5412 - accuracy: 0.9155 - 228ms/epoch - 16ms/step\n",
      "Epoch 84/100\n",
      "14/14 - 0s - loss: 0.5335 - accuracy: 0.9296 - 231ms/epoch - 17ms/step\n",
      "Epoch 85/100\n",
      "14/14 - 0s - loss: 0.5096 - accuracy: 0.9296 - 223ms/epoch - 16ms/step\n",
      "Epoch 86/100\n",
      "14/14 - 0s - loss: 0.4955 - accuracy: 0.9319 - 221ms/epoch - 16ms/step\n",
      "Epoch 87/100\n",
      "14/14 - 0s - loss: 0.4890 - accuracy: 0.9319 - 226ms/epoch - 16ms/step\n",
      "Epoch 88/100\n",
      "14/14 - 0s - loss: 0.4729 - accuracy: 0.9343 - 225ms/epoch - 16ms/step\n",
      "Epoch 89/100\n",
      "14/14 - 0s - loss: 0.4704 - accuracy: 0.9390 - 231ms/epoch - 17ms/step\n",
      "Epoch 90/100\n",
      "14/14 - 0s - loss: 0.4546 - accuracy: 0.9390 - 223ms/epoch - 16ms/step\n",
      "Epoch 91/100\n",
      "14/14 - 0s - loss: 0.4310 - accuracy: 0.9390 - 225ms/epoch - 16ms/step\n",
      "Epoch 92/100\n",
      "14/14 - 0s - loss: 0.4253 - accuracy: 0.9507 - 223ms/epoch - 16ms/step\n",
      "Epoch 93/100\n",
      "14/14 - 0s - loss: 0.4169 - accuracy: 0.9507 - 224ms/epoch - 16ms/step\n",
      "Epoch 94/100\n",
      "14/14 - 0s - loss: 0.4075 - accuracy: 0.9531 - 227ms/epoch - 16ms/step\n",
      "Epoch 95/100\n",
      "14/14 - 0s - loss: 0.3924 - accuracy: 0.9484 - 221ms/epoch - 16ms/step\n",
      "Epoch 96/100\n",
      "14/14 - 0s - loss: 0.3679 - accuracy: 0.9624 - 224ms/epoch - 16ms/step\n",
      "Epoch 97/100\n",
      "14/14 - 0s - loss: 0.3615 - accuracy: 0.9624 - 231ms/epoch - 17ms/step\n",
      "Epoch 98/100\n",
      "14/14 - 0s - loss: 0.3476 - accuracy: 0.9601 - 224ms/epoch - 16ms/step\n",
      "Epoch 99/100\n",
      "14/14 - 0s - loss: 0.3397 - accuracy: 0.9624 - 224ms/epoch - 16ms/step\n",
      "Epoch 100/100\n",
      "14/14 - 0s - loss: 0.3285 - accuracy: 0.9648 - 220ms/epoch - 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c948e460>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modeling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "hidden_units = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_units, input_shape=(X_train_ohe.shape[1], X_train_ohe.shape[2])))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train_ohe, y_train_ohe, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 19:51:18.510470: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-07-19 19:51:18.565965: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get on with life as a programmer, I like to use words about beer. But when I stapt to aa\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, char_to_idx, seq_len, seed_text, n):\n",
    "    \"\"\"문자열을 입력 받아 해당 문자열로부터 다음 문자를 예측하는 것을 반복하여 그 문장을 반환하는 함수\"\"\"\n",
    "    init_text = seed_text\n",
    "    sentence = \"\"\n",
    "\n",
    "    for _ in range(n): # n번만 예측\n",
    "        encoded = [char_to_idx[char] for char in seed_text] # 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_len, padding=\"pre\") # 패딩\n",
    "        encoded = to_categorical(encoded, num_classes=len(char_to_idx)) # OHE\n",
    "\n",
    "        res = model.predict(encoded, verbose=0)\n",
    "        res = np.argmax(res, axis=1)\n",
    "\n",
    "        for char, idx in char_to_idx.items():\n",
    "            if idx == res:\n",
    "                break\n",
    "        \n",
    "        seed_text = seed_text + char\n",
    "        sentence = sentence + char\n",
    "    sentence = init_text + sentence\n",
    "    return sentence\n",
    "\n",
    "print(generate_sentence(model, char_to_idx, 10, \"I get on w\", 80))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
